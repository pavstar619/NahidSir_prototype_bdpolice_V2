{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NahidSir_ACLED_Prototype.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwD+x8OY7oa05uzLPdQomw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavstar619/NahidSir_prototype_bdpolice_V2/blob/main/NahidSir_ACLED_Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXrIvym_FUqv"
      },
      "source": [
        "Note: \n",
        "\n",
        "* Acled xlsx event type had some different worded rows. Fixed that after making the first total counts xlsx\n",
        "\n",
        "* (SOLVED) Tried Specific events by separate years with cities. Didnt work out due to each city having unequal amount of events\n",
        "\n",
        "  * (SOLVED) Tried using dictionary but lots of empty arrays as one city might not have any crimes at all for one year. Not sure how to append dict to excel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChDHnWza714w",
        "outputId": "849b2052-813b-4b0a-cbd1-b19295551dd5"
      },
      "source": [
        "!pip install XlsxWriter\n",
        "!pip install scikit-multilearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: XlsxWriter in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNWsqwkyfoIJ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import folium\n",
        "\n",
        "from geopy.geocoders import Nominatim, OpenMapQuest\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_uDtTnpUAHu"
      },
      "source": [
        "# !pip install geopy\n",
        "# !pip install folium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z82cj9MeRU1",
        "outputId": "a66e47fe-a7c2-4dd1-a0e2-58cf39e84d08"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM8ZeogDfJas",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "2130b493-94ee-471d-8f42-9f50774f06bc"
      },
      "source": [
        "# data = pd.read_excel('ACLED_Data_Prototype.xlsx')\n",
        "data = pd.read_excel('/content/gdrive/MyDrive/Thesis Mates/Thesis Part 2/ACLED Data & Paper Writing Guideline /ACLED_Data_Prototype.xlsx')\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>YEAR</th>\n",
              "      <th>EVENT_TYPE</th>\n",
              "      <th>LOCATION</th>\n",
              "      <th>LATITUDE</th>\n",
              "      <th>LONGITUDE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>Battles</td>\n",
              "      <td>Lohagara</td>\n",
              "      <td>23.1998</td>\n",
              "      <td>89.6644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Hathazari</td>\n",
              "      <td>22.5052</td>\n",
              "      <td>91.8134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Rangamati</td>\n",
              "      <td>22.6432</td>\n",
              "      <td>92.1919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Rajoir</td>\n",
              "      <td>23.1878</td>\n",
              "      <td>90.0322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Rajshahi</td>\n",
              "      <td>24.3740</td>\n",
              "      <td>88.6011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30639</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Paba</td>\n",
              "      <td>24.4417</td>\n",
              "      <td>88.6278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30640</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Bhuapur</td>\n",
              "      <td>24.4604</td>\n",
              "      <td>89.8727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30641</th>\n",
              "      <td>2020</td>\n",
              "      <td>Protests</td>\n",
              "      <td>Dhaka</td>\n",
              "      <td>23.7406</td>\n",
              "      <td>90.3943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30642</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Bogra</td>\n",
              "      <td>24.8510</td>\n",
              "      <td>89.3711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30643</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Dhaka</td>\n",
              "      <td>23.7333</td>\n",
              "      <td>90.4000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30644 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       YEAR EVENT_TYPE   LOCATION  LATITUDE  LONGITUDE\n",
              "0      2001    Battles   Lohagara   23.1998    89.6644\n",
              "1      2001      Riots  Hathazari   22.5052    91.8134\n",
              "2      2001      Riots  Rangamati   22.6432    92.1919\n",
              "3      2001      Riots     Rajoir   23.1878    90.0322\n",
              "4      2001      Riots   Rajshahi   24.3740    88.6011\n",
              "...     ...        ...        ...       ...        ...\n",
              "30639  2020      Riots       Paba   24.4417    88.6278\n",
              "30640  2020      Riots    Bhuapur   24.4604    89.8727\n",
              "30641  2020   Protests     Dhaka    23.7406    90.3943\n",
              "30642  2020      Riots      Bogra   24.8510    89.3711\n",
              "30643  2020      Riots     Dhaka    23.7333    90.4000\n",
              "\n",
              "[30644 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn-e-2YfYEiL"
      },
      "source": [
        "# data.loc[0, 'LOCATION']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUlt8kD-ZR6E"
      },
      "source": [
        "Specific Event types total counts by year\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ9nzSsKhWBY"
      },
      "source": [
        "# years = data.YEAR.unique()\n",
        "# events = data.EVENT_TYPE.unique() # has duplicates same values actual values is 6 not 9\n",
        "# print(events)\n",
        "\n",
        "# totalcount_df = pd.DataFrame(columns = ['YEAR'])\n",
        "\n",
        "# for i in years:\n",
        "#   year = data.loc[data['YEAR'] == i]\n",
        "#   duplicates = year.pivot_table(index = ['EVENT_TYPE'], aggfunc ='size')\n",
        "#   print(i)\n",
        "#   print(duplicates,'\\n')\n",
        "\n",
        "#   totalcount_df = totalcount_df.append(duplicates, ignore_index=True)\n",
        "\n",
        "# totalcount_df\n",
        "\n",
        "# totalcount_df.to_csv('ACLED_totalcounts.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3V4dsYFvV1x"
      },
      "source": [
        "Total Events by year and cities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu7R9LnvnhCt"
      },
      "source": [
        "# years = data['YEAR'].unique()\n",
        "# locations = data['LOCATION'].unique()\n",
        "# events = data['EVENT_TYPE'].unique()\n",
        "\n",
        "# locations_totalcount_df = pd.DataFrame(columns = [years])\n",
        "# count = 0\n",
        "\n",
        "# for i in locations:\n",
        "#   for j in years:\n",
        "#     x = data.loc[(data['LOCATION'] == i) & (data['YEAR'] == j)]\n",
        "#     # duplicates = x.pivot_table(index = ['EVENT_TYPE'], aggfunc ='size') # enable this to show specific events like riots murder etc\n",
        "#     sum = x['EVENT_TYPE'].count()\n",
        "#     # print('~~~~~~~~~~~~~~~~~~~~~~')\n",
        "#     # print(i,j,'Total events: ',sum)\n",
        "#     # print(duplicates)\n",
        "\n",
        "#     locations_totalcount_df.loc[count,j] = sum\n",
        "\n",
        "#   count+=1\n",
        "\n",
        "# # locations_totalcount_df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIqKynl0wCHJ"
      },
      "source": [
        "# # Add cities to dataframe\n",
        "# locations_totalcount_df['LOCATION'] = locations.tolist()\n",
        "# locations_totalcount_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK8WJE3tu42I"
      },
      "source": [
        "# locations_totalcount_df.to_excel('ACLED_locations_totalcount_df.xlsx',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ek_EK1BhN2-"
      },
      "source": [
        "# x = data.loc[(data['LOCATION'] == 'Hathazari') & (data['YEAR'] == 2011)]\n",
        "# x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2mXTpqfb8GE"
      },
      "source": [
        "Specific Events by years by cities (using Dictionary -> excel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLCofKy-N4pq"
      },
      "source": [
        "# years = data['YEAR'].unique()\n",
        "# locations = data['LOCATION'].unique()\n",
        "# events = data['EVENT_TYPE'].unique()\n",
        "\n",
        "# years_dict = {}\n",
        "# for i in years:\n",
        "#   location_dict = {}\n",
        "#   for j in locations:\n",
        "#     x = data.loc[(data['LOCATION'] == j) & (data['YEAR'] == i)]\n",
        "#     duplicates = x.pivot_table(index = ['EVENT_TYPE'], aggfunc ='size') # enable this to show specific events like riots murder etc\n",
        "#     event_dict = {}\n",
        "#     for event, value in duplicates.items():\n",
        "#       event_dict[event] = value \n",
        "\n",
        "#     location_dict[j] = event_dict\n",
        "\n",
        "#   years_dict[i] = location_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3OLeI-tPODg"
      },
      "source": [
        "# for i in years_dict:\n",
        "#   print(years_dict[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAI2S5FKSnZv"
      },
      "source": [
        "# writer = pd.ExcelWriter('yearwise_locations_specificevent.xlsx', engine='xlsxwriter')\n",
        "\n",
        "# yearwise_locations_specificevent = pd.DataFrame()\n",
        "# yearwise_locations_specificevent\n",
        "\n",
        "# for year in years_dict:\n",
        "#   count = 0\n",
        "#   for city in years_dict[year]:\n",
        "#     yearwise_locations_specificevent.loc[count, 'LOCATION'] = city\n",
        "\n",
        "#     x = years_dict[year][city]\n",
        "#     for event,value in x.items():\n",
        "#       yearwise_locations_specificevent.loc[count, event] = value\n",
        "#       # yearwise_locations_specificevent[str(event)].iloc[count] = value\n",
        "\n",
        "#     count+=1\n",
        "\n",
        "#     # for event in years_dict[year][city]:\n",
        "\n",
        "\n",
        "#   yearwise_locations_specificevent.to_excel(writer, sheet_name=str(year),index=False)\n",
        "\n",
        "# writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmuplMZkj4x0"
      },
      "source": [
        "Specific Events by Event Type by cities (using Dictionary -> excel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxZE0WWFkF70"
      },
      "source": [
        "# years = data['YEAR'].unique()\n",
        "# locations = data['LOCATION'].unique()\n",
        "# events = data['EVENT_TYPE'].unique()\n",
        "\n",
        "# specific_event_type_totalcount = pd.DataFrame()\n",
        "\n",
        "# writer = pd.ExcelWriter('specific_event_type_totalcount.xlsx', engine='xlsxwriter')\n",
        "\n",
        "# for k in events:\n",
        "#   count = 0\n",
        "#   for i in locations:\n",
        "#   # for j in years:\n",
        "#     # x = data.loc[(data['LOCATION'] == i) & (data['YEAR'] == j) & (data['EVENT_TYPE'] == k)]\n",
        "#     x = data.loc[(data['LOCATION'] == i) & (data['EVENT_TYPE'] == k)]\n",
        "#     # duplicates = x.pivot_table(index = ['EVENT_TYPE'], aggfunc ='size') # enable this to show specific events like riots murder etc\n",
        "#     sum = x['EVENT_TYPE'].count()\n",
        "#     # print('~~~~~~~~~~~~~~~~~~~~~~')\n",
        "#     # print(i,j,'Total events: ',sum)\n",
        "#     # print(duplicates)\n",
        "      \n",
        "\n",
        "#     specific_event_type_totalcount.loc[count,'Total'] = sum\n",
        "\n",
        "#     count+=1\n",
        "\n",
        "#   specific_event_type_totalcount.to_excel(writer, sheet_name=str(k),index=False)\n",
        "\n",
        "# writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Xzxe8CUd0i"
      },
      "source": [
        "Geo Mapping Stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2UdR40E71LG"
      },
      "source": [
        "1. Geo Map by all locations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYqqwRdlUdf-"
      },
      "source": [
        "# plt.figure(figsize = (15,8))\n",
        "# sns.scatterplot(data['LATITUDE'], data['LONGITUDE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChlAL6--WheF"
      },
      "source": [
        "# lat_median = data['LATITUDE'].median() # y\n",
        "# long_median = data['LONGITUDE'].median() # x\n",
        "# print(lat_median,long_median)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HehUVH2sVhi8"
      },
      "source": [
        "# # create the map.\n",
        "# map_pickup = folium.Map(location=[lat_median, long_median], zoom_start = 6)\n",
        "\n",
        "# # adding the latitude and longitude points to the map.\n",
        "# data.apply(lambda row:folium.Circle(location=[row['LATITUDE'], row['LONGITUDE']], radius = 0.25).add_to(map_pickup), axis=1)\n",
        "\n",
        "# # optional: save the map.\n",
        "# # map_pickup.save('geomap_bd.html')\n",
        "\n",
        "# # display the map: just ask for the object representation in juypter notebook.\n",
        "# map_pickup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk4jh_Ob0fSJ"
      },
      "source": [
        "Geo Locater find districts and stuff\n",
        "\n",
        "Problems:\n",
        " * Takes too long for each of the 30000 rows (2rows/sec speed)\n",
        " * (Temp Fixed). Use 1380 rows of unique locations instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s83LL7IjvtBp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ae0f1d-8f18-4e1f-b157-76b8707f024e"
      },
      "source": [
        "# create the locator\n",
        "geolocator = Nominatim(user_agent=\"GGWP\") # 120 req per min\n",
        "\n",
        "# getting the location address\n",
        "location = geolocator.reverse((22.3075,89.0981))\n",
        "# location = geolocator.geocode('Dhaka')\n",
        "\n",
        "print(location.raw)\n",
        "# print(location.raw['address']['state_district'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'place_id': 227123463, 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright', 'osm_type': 'way', 'osm_id': 679014761, 'lat': '22.3049522374524', 'lon': '89.09780493822058', 'display_name': 'Ramjan Nagar Road, খুলনা বিভাগ, বাংলাদেশ', 'address': {'road': 'Ramjan Nagar Road', 'state': 'খুলনা বিভাগ', 'country': 'বাংলাদেশ', 'country_code': 'bd'}, 'boundingbox': ['22.3045414', '22.3056459', '89.0977949', '89.1062559']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "162zpaCzBZNW"
      },
      "source": [
        "\n",
        "# from geopy.geocoders import OpenMapQuest\n",
        "\n",
        "# point = '23.1998, 89.6644' #here's famous Sherlock Holmes' museum lat & lng\n",
        "\n",
        "# geolocator = OpenMapQuest(api_key='x5LPBPjUCj9CUEl4U9fUrnoLGfGv7SWk')\n",
        "# address = geolocator.reverse(point)\n",
        "# print(address[0]) # use other indexes if you want more or less detailed address scope."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUqU8UteBlE0"
      },
      "source": [
        "# # # https://stackoverflow.com/questions/31506272/geopy-too-slow-timeout-all-the-time\n",
        "# # # TIME OUT ERROR FIX\n",
        "\n",
        "# # GEO CODER FUNCTION\n",
        "# def geocode(data,i, recursion=0):\n",
        "#     # print((data.loc[i, 'LATITUDE'], data.loc[i, 'LONGITUDE']))\n",
        "#     try:\n",
        "#         return geolocator.reverse((data.loc[i, 'LATITUDE'], data.loc[i, 'LONGITUDE']))\n",
        "#     except GeocoderTimedOut as e:\n",
        "#         if recursion > 10:      # max recursions\n",
        "#             raise e\n",
        "\n",
        "#         time.sleep(5) # wait a bit\n",
        "#         # try again\n",
        "#         return geocode(i, recursion=recursion + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ2Ix1Kp3zul"
      },
      "source": [
        "# # GEO LOCATOR FUNCTION\n",
        "# def locator(data,excelname):\n",
        "#   for i in range(len(data)):\n",
        "#     location = geolocator.reverse((data.loc[i, 'LATITUDE'], data.loc[i, 'LONGITUDE']))\n",
        "#     location = geocode(data,i)\n",
        "#     # print(location.raw)\n",
        "#     try:\n",
        "#       # all districts\n",
        "#       if location.raw['address'].__contains__('county'):\n",
        "#         data.loc[i, 'DISTRICT'] = location.raw['address']['county'] # for county key\n",
        "#       elif location.raw['address'].__contains__('state_district'):\n",
        "#         data.loc[i, 'DISTRICT'] = location.raw['address']['state_district'] # for state district key\n",
        "#       elif location.raw['address'].__contains__('city'):\n",
        "#         data.loc[i, 'DISTRICT'] = location.raw['address']['city'] \n",
        "#       else:\n",
        "#         substring = \"জেলা\" # check if substring in state value\n",
        "#         if substring in location.raw['address']['state']:\n",
        "#           data.loc[i, 'DISTRICT'] = location.raw['address']['state'] \n",
        "\n",
        "#       # all divisions\n",
        "#       # data.loc[i, 'DIVISION'] = location.raw['address']['state']\n",
        "#     except KeyError:\n",
        "#       print('not found')\n",
        "#     print(i)\n",
        "\n",
        "#   data.to_excel(excelname,index=False)\n",
        "#   data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x9ZaT427_gX"
      },
      "source": [
        "2. Geo Map by divisions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL0epL5mI2MO"
      },
      "source": [
        "# x = pd.DataFrame()\n",
        "# x = data.drop_duplicates(['LOCATION'])[['LOCATION','LATITUDE','LONGITUDE']]\n",
        "# x = x.reset_index(drop=True)\n",
        "# x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48JRijaqEKLI"
      },
      "source": [
        "# locator(x, 'location_by_division.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1aPKvfOOZyg"
      },
      "source": [
        "# RUN FOREVER WITHOUT DISCONNECT \n",
        "# while True:pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSqgH3F4sI-o"
      },
      "source": [
        "Classification Note:\n",
        " * Predict using Year,City - Find coords and event type(idk how)\n",
        " * Predict using Year, City - Find only event type (multiple events in one city how to handle?) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf31jjEcd2-N"
      },
      "source": [
        "1. Classify years with events only\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-WXIHVPgpYV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import linear_model \n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqX6QNXd2q_"
      },
      "source": [
        "# data = pd.read_excel('/content/gdrive/MyDrive/Thesis Mates/Thesis Part 2/ACLED Data & Paper Writing Guideline /Data Outputs/ACLED_event_totalcounts.xlsx')\n",
        "# data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "552OZvsSfhFC"
      },
      "source": [
        "# def R2_calc(pred_y, test_y): # Closer to 1 better score\n",
        "#   return r2_score(pred_y, test_y)\n",
        "\n",
        "# def MSE_calc(pred_y, test_y): # Lower the better\n",
        "#   return mean_squared_error(pred_y, test_y, squared = False)\n",
        "  \n",
        "# def MAE_calc(pred_y, test_y): # Lower the better\n",
        "#   return mean_absolute_error(pred_y, test_y)\n",
        "\n",
        "# def Error_calc(pred_y, test_y):\n",
        "#   return (pred_y - test_y) / test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4TkXcc0fUJw"
      },
      "source": [
        "# def modelizeall(dmp, dmp_2018):\n",
        "#   # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "#   writer = pd.ExcelWriter('ACLED_event_totalcounts_test_pred_data.xlsx', engine='xlsxwriter')\n",
        "#   # writer1 = pd.ExcelWriter('Without2018train_errors_data.xlsx', engine='xlsxwriter')\n",
        "\n",
        "#   model = Sequential()\n",
        "#   model.add(Dense(8, input_dim=1, activation='relu'))\n",
        "#   model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
        "#   model.add(Dense(1, kernel_initializer='normal'))\n",
        "#   # used binary loss function \n",
        "#   model.compile(loss='mse', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "#   # print(model.summary())\n",
        "\n",
        "#   # dmp_x = dmp.loc[:, dmp.columns != 'Robbery']\n",
        "#   # dmp_y = dmp['Robbery']\n",
        "#   for col in dmp.columns[1:].unique():\n",
        "#     print('~~~~~~~~~~~~~~~~~~~~~', col, '~~~~~~~~~~~~~~~~~~~~~')\n",
        "#     dmp_x = dmp['YEAR'].values\n",
        "#     dmp_y = dmp[col].values\n",
        "\n",
        "#     # ASSIGN MANUALLY\n",
        "#     train_x = dmp_x\n",
        "#     train_y = dmp_y\n",
        "\n",
        "#     test_x = dmp_2018['YEAR'].values\n",
        "#     test_y = dmp_2018[col].values\n",
        "\n",
        "\n",
        "#     # train_x, test_x, train_y, test_y = train_test_split(dmp_x, dmp_y, test_size = 0.25, random_state=0)\n",
        "\n",
        "#     # Standard scaling\n",
        "#     ss = StandardScaler()\n",
        "#     ss.fit(train_x.reshape(-1, 1))\n",
        "#     alpha_train_x = ss.transform(train_x.reshape(-1, 1))\n",
        "#     ss_test_x = ss.transform(test_x.reshape(-1, 1))\n",
        "\n",
        "#     # print(train_x)\n",
        "#     # print(test_x)\n",
        "\n",
        "\n",
        "\n",
        "#     # pred for single data\n",
        "#     Xnew = [[2021]]\n",
        "#     # print(len(Xnew[0]))\n",
        "#     # print(len(dmp_x.columns))\n",
        "\n",
        "#     regressor = DecisionTreeRegressor(random_state=0)\n",
        "#     forest = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "#     svr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
        "#     mlp = MLPClassifier(random_state=0, max_iter = 2000)\n",
        "#     lasso = linear_model.Lasso(alpha=0.1)\n",
        "#     bayesian = linear_model.BayesianRidge()\n",
        "#     ridge = linear_model.Ridge()\n",
        "#     lr = LinearRegression()\n",
        "\n",
        "#     regressor.fit(train_x.reshape(-1, 1),train_y)  \n",
        "#     forest.fit(train_x.reshape(-1, 1),train_y) \n",
        "#     svr.fit(train_x.reshape(-1, 1),train_y)\n",
        "#     mlp.fit(alpha_train_x.reshape(-1,1),train_y) \n",
        "#     lasso.fit(train_x.reshape(-1,1),train_y) \n",
        "#     bayesian.fit(train_x.reshape(-1,1),train_y) \n",
        "#     ridge.fit(train_x.reshape(-1,1),train_y) \n",
        "#     lr.fit(train_x.reshape(-1,1),train_y)\n",
        "#     model.fit(train_x,train_y, epochs=30, batch_size=2)\n",
        "\n",
        "\n",
        "\n",
        "#     pred_y = regressor.predict(test_x.reshape(-1, 1))\n",
        "#     pred_y1 = forest.predict(test_x.reshape(-1, 1)) \n",
        "#     pred_y2 = svr.predict(test_x.reshape(-1, 1)) \n",
        "#     pred_y3 = mlp.predict(ss_test_x.reshape(-1, 1))\n",
        "#     pred_y4 = lasso.predict(test_x.reshape(-1,1))\n",
        "#     pred_y5 = bayesian.predict(test_x.reshape(-1,1))\n",
        "#     pred_y6 = ridge.predict(test_x.reshape(-1,1))\n",
        "#     pred_y7 = lr.predict(test_x.reshape(-1,1))\n",
        "#     pred_y8 = model.predict(test_x.reshape(-1,1)).tolist()\n",
        "\n",
        "#     # error_y = (pred_y - test_y) / test_y #lower the better\n",
        "#     # error_y1 = (pred_y1 - test_y) / test_y\n",
        "#     # error_y2 = (pred_y2 - test_y) / test_y\n",
        "#     # error_y3 = (pred_y3 - test_y) / test_y\n",
        "#     # error_y4 = (pred_y4 - test_y) / test_y\n",
        "#     # error_y5 = (pred_y5 - test_y) / test_y\n",
        "#     # error_y6 = (pred_y6 - test_y) / test_y\n",
        "#     # error_y7 = (pred_y7 - test_y) / test_y\n",
        "\n",
        "#     pred_y_list = [pred_y,pred_y1,pred_y2,pred_y3,pred_y4,pred_y5,pred_y6,pred_y7,pred_y8]\n",
        "#     error_y_list = []\n",
        "#     r2_y_list = []\n",
        "#     mse_y_list = []\n",
        "#     mae_y_list = []\n",
        "\n",
        "#     for pred_y in pred_y_list:\n",
        "#       # error_y = Error_calc(pred_y,test_y)\n",
        "#       # error_y_list.append(error_y)\n",
        "\n",
        "#       r2_y = R2_calc(pred_y,test_y)\n",
        "#       r2_y_list.append(r2_y)\n",
        "\n",
        "#       mse_y = MSE_calc(pred_y,test_y)\n",
        "#       mse_y_list.append(mse_y)\n",
        "\n",
        "#       mae_y = MAE_calc(pred_y,test_y)\n",
        "#       mae_y_list.append(mae_y)\n",
        "\n",
        "#     Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "#     model_list = ['Decision Tree', 'Random Forest', 'SVR', 'MLP(Adam)', 'Lasso', 'Bayesian', 'Ridge', 'Linear Regression','Neural Net']\n",
        "#     # print('~~~~ Test Val || Predicted Val || Errors: R2, RMSE, MAE ~~~~')\n",
        "#     # for i in range(len(pred_y_list)):\n",
        "#     #   print(model_list[i], 'Test Val: ',test_y, 'Predicted Val: ', pred_y_list[i], 'Errors R2 | RMSE | MAE: ', r2_y_list[i], mse_y_list[i], mae_y_list[i]) # Dont show Metric as only one test row\n",
        "\n",
        "\n",
        "#     # # Put in dataframes\n",
        "#     # errors_data = pd.DataFrame(list(zip(model_list, r2_y_list, mse_y_list, mae_y_list)),\n",
        "#     #            columns =['Algorithms','R2 Score','Mean Square Error(MSE)','Mean Absolute Error(MAE)'])\n",
        "#     # # print(errors_data)\n",
        "#     test_pred_data = pd.DataFrame(columns =['Algorithms','Test Years','Test Values','Predicted Values','R2 Score','Mean Square Error(MSE)','Mean Absolute Error(MAE)'])\n",
        "#     for i in range(len(pred_y_list)):\n",
        "#       test_pred_data.loc[i] = [model_list[i], test_x, test_y, list(np.around(pred_y_list[i],2)), r2_y_list[i], mse_y_list[i], mae_y_list[i]]\n",
        "#     # print(test_pred_data)\n",
        "\n",
        "\n",
        "#     # Put in excel sheets\n",
        "#     # Write each dataframe to a different worksheet.\n",
        "#     test_pred_data.to_excel(writer, sheet_name=col,index=False)\n",
        "#     # errors_data.to_excel(writer1, sheet_name=col,index=False)\n",
        "\n",
        "\n",
        "#     # Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "\n",
        "#     # m2_y = mean_squared_error(test_y,pred_y, squared=False)\n",
        "\n",
        "#     # print('~~~~ Test Val || Predicted Val || Errors: R2, RMSE, RSE ~~~~')\n",
        "#     # # print(test_y)\n",
        "#     # print('Test DT: ',test_y,pred_y,error_y) # Decision Tree\n",
        "#     # print('Test RF: ',test_y,pred_y1,error_y1) # Random Forest\n",
        "#     # print('Test SVR: ',test_y,pred_y2,error_y2) # SVM\n",
        "#     # print('Test MLP: ',test_y,pred_y3,error_y3) # mlp\n",
        "#     # print('Test Lasso: ',test_y,pred_y4,error_y4) # lasso\n",
        "#     # print('Test Bayesian: ',test_y,pred_y5,error_y5) # bayesian\n",
        "#     # print('Test Ridge: ',test_y,pred_y6,error_y6) # ridge\n",
        "#     # print('Test Linear Regression: ',test_y,pred_y7,error_y7) # lr\n",
        "\n",
        "#     # print('2018 BEST MLP: ',test_y,pred_y3) # Decision Tree\n",
        "#     print(Xnew,'BEST MLP: ',col,Xpred_y) # Decision Tree\n",
        "\n",
        "\n",
        "#   # Close the Pandas Excel writer and output the Excel file.\n",
        "#   writer.save()\n",
        "#   # writer1.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRZZnCMlfNFr"
      },
      "source": [
        "# # 2018 dmp\n",
        "# dmp_2018 = data.iloc[19:] # 2020\n",
        "# print(dmp_2018)\n",
        "\n",
        "# # 2010 to 2017\n",
        "# dmp_till2017 = data.drop(19)\n",
        "# print(dmp_till2017)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU46Yow0j-WJ"
      },
      "source": [
        "# modelizeall(dmp_till2017, dmp_2018)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz5xZ01U3DQr"
      },
      "source": [
        "1.1 Predict 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEDWRGeakpHA"
      },
      "source": [
        "# dmp = data\n",
        "# # MODELIZE 2021\n",
        "# for col in data.columns[1:].unique():\n",
        "#   print('~~~~~~~~~~~~~~~~~~~~~', col, '~~~~~~~~~~~~~~~~~~~~~')\n",
        "#   dmp_x = dmp['YEAR'].values\n",
        "#   dmp_y = dmp[col].values\n",
        "\n",
        "#   # ASSIGN MANUALLY\n",
        "#   train_x = dmp_x\n",
        "#   train_y = dmp_y\n",
        "\n",
        "#   # Standard scaling\n",
        "#   ss = StandardScaler()\n",
        "#   ss.fit(train_x.reshape(-1, 1))\n",
        "#   ss_train_x = ss.transform(train_x.reshape(-1, 1))\n",
        "\n",
        "\n",
        "#   # pred for single data\n",
        "#   Xnew = [[2021]]\n",
        "#   # print(len(Xnew[0]))\n",
        "#   # print(len(dmp_x.columns))\n",
        "\n",
        "#   mlp = MLPClassifier(random_state=0, max_iter = 5000)\n",
        "#   mlp.fit(ss_train_x.reshape(-1,1),train_y) \n",
        "\n",
        "#   Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "#   print(Xnew,'BEST MLP: ',col,Xpred_y) # Decision Tree\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM0KUqN528Xf"
      },
      "source": [
        "2. Multi Label Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD2ZupaKtTIP"
      },
      "source": [
        "Did: \n",
        "* Predicted Event \n",
        "* Predicted Lat,long \n",
        "* Predicted Location\n",
        "\n",
        "Left: \n",
        "* Predict Lat, Long, Event\n",
        "\n",
        "Problem:\n",
        "* Low precisions\n",
        "\n",
        "BREAKTHROUGH:\n",
        "* (FAILED cant read y data) IMPROVED MSE from 0.95 to 0.18 by scaling the y data. R2 from 0.2 to 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXXnaYbjL2Ud"
      },
      "source": [
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-nxHHDB3KxR"
      },
      "source": [
        "# data = pd.read_excel('/content/gdrive/MyDrive/Thesis Mates/Thesis Part 2/ACLED Data & Paper Writing Guideline /ACLED_Data_Prototype.xlsx')\n",
        "# data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp5jIDTlp712"
      },
      "source": [
        "2.1 Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7_hwtDM3rLU"
      },
      "source": [
        "# x = data.copy()\n",
        "# x = x.drop('YEAR',axis=1)\n",
        "# x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_crY-gUTL6LN"
      },
      "source": [
        "# x = data[['YEAR','EVENT_TYPE','LOCATION']]\n",
        "# # x = data[['YEAR','LOCATION','LATITUDE','LONGITUDE']]\n",
        "# # x = x.drop('EVENT_TYPE', axis = 1)\n",
        "# y = data[['LATITUDE','LONGITUDE']]\n",
        "# # y = data[['EVENT_TYPE']]\n",
        "\n",
        "# # x = data.loc[:, data.columns != ('EVENT_TYPE',)]\n",
        "# # y = data.loc[:, data.columns == 'EVENT_TYPE']\n",
        "\n",
        "# print(x)\n",
        "# print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxjVoeVW-hQW"
      },
      "source": [
        "# # USE THIS OTHERWISE LE.FIT CAUSES PROBLEMS\n",
        "# unique_events = data['EVENT_TYPE'].unique()\n",
        "# unique_locations = data['LOCATION'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca56odIXlnqK"
      },
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# from sklearn.linear_model import SGDClassifier\n",
        "# from lightgbm import LGBMClassifier\n",
        "# from sklearn.ensemble import AdaBoostClassifier\n",
        "# from xgboost import XGBClassifier\n",
        "\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.neural_network import MLPRegressor\n",
        "# from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# from sklearn import preprocessing\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# from pandas.api.types import is_string_dtype\n",
        "# from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "\n",
        "# # x_numpy = x.to_numpy()\n",
        "# # y_numpy = y.to_numpy()\n",
        "\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
        "# # stratify=y_numpy\n",
        "\n",
        "\n",
        "# # MULTI OUTPUT REGRESSION MODELS: LinearRegression, KNeighborsRegressor, DecisionTreeRegressor,RandomForestRegressor\n",
        "\n",
        "# models = [\n",
        "#           ('MLP', MLPRegressor(random_state=0, max_iter = 2000)),\n",
        "#           # ('Linear Regression', LinearRegression()),\n",
        "#           # ('Random Forest', RandomForestClassifier()),\n",
        "#           # ('Decision Tree', DecisionTreeClassifier()),\n",
        "#           # ('SVM', SVC()),\n",
        "#           # ('KNN', KNeighborsClassifier()),\n",
        "#           # ('Naive Bayes',GaussianNB()),\n",
        "#           # ('Logistics Regression', LogisticRegression()),\n",
        "#           # ('Stochastic Gradient Descent', SGDClassifier()),\n",
        "#           # ('LightGBM', LGBMClassifier()),\n",
        "#           # ('AdaBoost', AdaBoostClassifier()),\n",
        "#           # ('XGBoost', XGBClassifier())\n",
        "#           ]\n",
        "\n",
        "# # single  = [2021, 15, 24, 89]\n",
        "\n",
        "# ms = MinMaxScaler()\n",
        "# # ms = StandardScaler()\n",
        "# le = LabelEncoder()\n",
        "\n",
        "# # le.fit(train_y)\n",
        "# # train_y = le.transform(train_y)\n",
        "# # test_y = le.transform(test_y)\n",
        "\n",
        "# ms_train_x = pd.DataFrame()\n",
        "# ms_test_x = pd.DataFrame()\n",
        "\n",
        "# # LABEL ENCODER + Min max Scaler\n",
        "# for col in x_train.columns:\n",
        "#   if (is_string_dtype(x_train[col].dtype)):\n",
        "#     uniques = data[col].unique() # Do this otherwise test doesnt get enough labels to labelencode\n",
        "#     le.fit(uniques)\n",
        "#     ms_train_x[col] = le.transform(x_train[[col]]).flatten() # flatten otherwise 1d doesnt work as numpy array\n",
        "#     ms_test_x[col] = le.transform(x_test[[col]]).flatten()\n",
        "\n",
        "#   else:\n",
        "#     ms.fit(x_train[[col]])\n",
        "#     ms_train_x[col] = ms.transform(x_train[[col]]).flatten() # flatten otherwise 1d doesnt work as numpy array\n",
        "#     ms_test_x[col] = ms.transform(x_test[[col]]).flatten()\n",
        "\n",
        "# print(ms_train_x)\n",
        "# # print(ms_test_x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ useless code # all to use\n",
        "# # # PREPROCESS LOCATION\n",
        "# # le = LabelEncoder()\n",
        "\n",
        "# # le.fit(unique_events)\n",
        "# # x_train[:,1] = le.transform(x_train[:,1]) # Events\n",
        "# # x_test[:,1] = le.transform(x_test[:,1])\n",
        "# # print(le.classes_)\n",
        "\n",
        "\n",
        "# # le.fit(unique_locations)\n",
        "# # x_train[:,2] = le.transform(x_train[:,2]) # Locations\n",
        "# # x_test[:,2] = le.transform(x_test[:,2])\n",
        "# # print(le.classes_)\n",
        "# # # x_train.loc[:, ('LOCATION')] = le.transform(x_train.loc[:, ('LOCATION')]) # IMPORTANT: USE .loc instead of direct ['LOCATION'] otherwise chain indexing error\n",
        "# # # x_test.loc[:, ('LOCATION')] = le.transform(x_test.loc[:, ('LOCATION')])\n",
        "# # # print(x['LOCATION'])\n",
        "# # # single[1] = le.transform(single[1])\n",
        "\n",
        "\n",
        "# # # df = pd.DataFrame(single)\n",
        "# # # df.loc[1] = le.transform(df.loc[1].values)\n",
        "# # # single = df.values.tolist()\n",
        "\n",
        "\n",
        "# # # PREPROCESS Y LABELS\n",
        "# # # le2 = LabelEncoder()\n",
        "# # # le2.fit(unique_events)\n",
        "# # # y_train = le2.transform(y_train)\n",
        "# # # y_test = le2.transform(y_test)\n",
        "# # # print(le2.classes_)\n",
        "\n",
        "# # # STANDARD SCALE X Attributes\n",
        "# # ss = StandardScaler()\n",
        "# # ss.fit(x_train)\n",
        "# # x_train = ss.transform(x_train)\n",
        "# # x_test = ss.transform(x_test)\n",
        "# # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ useless code\n",
        "\n",
        "\n",
        "\n",
        "# # df = ss.transform(df)\n",
        "# # single = df.values.tolist()\n",
        "\n",
        "# for name, model in models:\n",
        "#   clf = model.fit(ss_train_x, y_train)\n",
        "#   predictions = clf.predict(ss_test_x)\n",
        "\n",
        "\n",
        "#   # evaluate multioutput regression model with k-fold cross-validation\n",
        "#   from numpy import absolute\n",
        "#   from numpy import mean\n",
        "#   from numpy import std\n",
        "#   from sklearn.datasets import make_regression\n",
        "#   from sklearn.tree import DecisionTreeRegressor\n",
        "#   from sklearn.model_selection import cross_val_score\n",
        "#   from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "\n",
        "#   # single_pred = clf.predict([single])\n",
        "\n",
        "#   from sklearn.metrics import classification_report\n",
        "#   from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "#   print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "#   print(name)\n",
        "#   # print(single_pred)\n",
        "#   print(predictions)\n",
        "#   print('R2 Score', r2_score(y_test, predictions))\n",
        "#   print('MSE', mean_squared_error(y_test, predictions, squared= False))\n",
        "#   # print('Precision | Recall | F1 Score | Support')\n",
        "#   # print(precision_recall_fscore_support(y_test, predictions, average='weighted', labels=np.unique(predictions))) # why did i use np.unique()??\n",
        "#   # print(classification_report(y_test,predictions, target_names=(le.classes_).tolist()))\n",
        "#   # print(classification_report(y_test,predictions, target_names=(le.classes_).tolist()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqu3LVzEp071"
      },
      "source": [
        "2.2 Whole data as input: Did regr and clf separate predictions same input\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtbSFFzlqiq6"
      },
      "source": [
        "# data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XegsC3asehVu"
      },
      "source": [
        "# x = data[['YEAR','LOCATION']]\n",
        "# y1 = data[['LATITUDE','LONGITUDE']]\n",
        "# y2 = data[['EVENT_TYPE']]\n",
        "# print(x)\n",
        "# print(y1)\n",
        "# print(y2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWtsxxTJrHZn"
      },
      "source": [
        "# model_regr = MLPRegressor(random_state=0, max_iter = 2000)\n",
        "# model_clf = MLPClassifier(random_state=0, max_iter = 2000)\n",
        "\n",
        "# ms_train_x = pd.DataFrame()\n",
        "\n",
        "# # # LABEL ENCODER + Min max Scaler Training Data\n",
        "# for col in x.columns:\n",
        "#   if (is_string_dtype(x[col].dtype)):\n",
        "#     uniques = data[col].unique() # Do this otherwise test doesnt get enough labels to labelencode\n",
        "#     le.fit(uniques)\n",
        "#     ms_train_x[col] = le.transform(x[[col]]).flatten() # flatten otherwise 1d doesnt work as numpy array\n",
        "#   else:\n",
        "#     # ms_train_x[col] = x[col].copy() # flatten otherwise 1d doesnt work as numpy array\n",
        "#     ms.fit(x[[col]])\n",
        "#     ms_train_x[col] = ms.transform(x[[col]]).flatten() # flatten otherwise 1d doesnt work as numpy array\n",
        "\n",
        "# Xnew = [[2021, 'Dhaka'],[2021,'Barisal']]\n",
        "# df_Xnew = pd.DataFrame(Xnew, columns = ['YEAR', 'LOCATION'])\n",
        "\n",
        "# ms_df_Xnew = pd.DataFrame()\n",
        "\n",
        "# # # LABEL ENCODER + Min max Scaler Single Datas\n",
        "# for col in df_Xnew.columns:\n",
        "#   if (is_string_dtype(df_Xnew[col].dtype)):\n",
        "#     ms_df_Xnew[col] = le.transform(df_Xnew[[col]]).flatten() # flatten otherwise 1d doesnt work as numpy array\n",
        "#   else:\n",
        "#     # ms_train_x[col] = x[col].copy() # flatten otherwise 1d doesnt work as numpy array\n",
        "#     ms_df_Xnew[col] = ms.transform(df_Xnew[[col]]).flatten() # flatten otherwise 1d doesnt work as numpy array\n",
        "\n",
        "# # Label Encoder Test Data \n",
        "# uniques = y2['EVENT_TYPE'].unique()\n",
        "# le.fit(uniques)\n",
        "# le_y2 = le.transform(y2['EVENT_TYPE'])\n",
        "# # Xnew[1] = le.transform(Xnew[1])\n",
        "# # print(le_y2)\n",
        "\n",
        "# # print(ms_train_x)\n",
        "# # print(y1)\n",
        "# # print(y2)\n",
        "# model_regr.fit(ms_train_x, y1)\n",
        "# model_clf.fit(ms_train_x,y2)\n",
        "\n",
        "# pred_regr = model_regr.predict(ms_df_Xnew)\n",
        "# pred_clf = model_clf.predict(ms_df_Xnew)\n",
        "\n",
        "# for i in range(len(pred_regr)):\n",
        "#   print('Test Data: ',Xnew[i])\n",
        "#   print('Predicted Data: ',pred_regr[i],pred_clf[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr5L05wkZ7ip"
      },
      "source": [
        "Problem:\n",
        " * Only 5 labels shown but should be 6 events\n",
        "\n",
        "Ideas:\n",
        "* try adding more col in train y\n",
        "* do the 3 col preds separtely or coord at once then append result\n",
        "* do the lat long first. Add to dataset then do the event type\n",
        "\n",
        "Solve: \n",
        "* 2 ways to solve this https://machinelearningmastery.com/multi-output-regression-models-with-python/. Another problem arises since event type is classification and location is regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdXNrp24bONZ"
      },
      "source": [
        "# x = data.loc[:, data.columns != 'EVENT_TYPE']\n",
        "# y = data['EVENT_TYPE']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fou5Oiq0Nvey"
      },
      "source": [
        "# data['EVENT_TYPE'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QskWuIlJM20w"
      },
      "source": [
        "# # x, y = make_multilabel_classification(n_samples = len(data['EVENT_TYPE'].unique()), random_state=0,return_indicator=False)\n",
        "# # this will generate a random multi-label dataset\n",
        "# x, y = make_multilabel_classification(sparse = True, n_labels = 6, return_indicator = 'sparse', allow_unlabeled = False)\n",
        "# # print(x)\n",
        "# # print(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awc95A1YPT_J"
      },
      "source": [
        "# # using binary relevance\n",
        "# from skmultilearn.problem_transform import BinaryRelevance\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# # initialize binary relevance multi-label classifier\n",
        "# # with a gaussian naive bayes base classifier\n",
        "# classifier = BinaryRelevance(GaussianNB())\n",
        "\n",
        "# # train\n",
        "# classifier.fit(x_train, y_train)\n",
        "\n",
        "# # predict\n",
        "# predictions = classifier.predict(x_test)\n",
        "# print(predictions.toarray())\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "# print(classification_report(y_test.toarray(),predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QHwkYjcRZ84"
      },
      "source": [
        "3. Predict Total Events by district"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cik51HxdRmJB"
      },
      "source": [
        "Instructions:\n",
        "1. Join Districts\n",
        "2. Drop Rows of West Bengal\n",
        "3. Input Year, District, Event Type. Output Lat,long, Total Count Specific Event"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "WbqKu4EFRZw5",
        "outputId": "c86dd44e-21fa-4f14-9c2d-45fca947d0c8"
      },
      "source": [
        "# DISTRICTS\n",
        "data2 = pd.read_excel('/content/gdrive/MyDrive/Thesis Mates/Thesis Part 2/ACLED Data & Paper Writing Guideline /Data Outputs/ACLED_location_by_district_V2.xlsx')\n",
        "data2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LOCATION</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>TRANSLATED DISTRICT</th>\n",
              "      <th>CAPITALIZED</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lohagara</td>\n",
              "      <td>নড়াইল</td>\n",
              "      <td>Narail</td>\n",
              "      <td>NARAIL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hathazari</td>\n",
              "      <td>চট্টগ্রাম</td>\n",
              "      <td>Chittagong</td>\n",
              "      <td>CHITTAGONG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Rangamati</td>\n",
              "      <td>রাঙ্গামাটি পার্বত্য</td>\n",
              "      <td>Rangamati Hill</td>\n",
              "      <td>RANGAMATI HILL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Rajoir</td>\n",
              "      <td>মাদারীপুর</td>\n",
              "      <td>Madaripur</td>\n",
              "      <td>MADARIPUR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Rajshahi</td>\n",
              "      <td>রাজশাহী</td>\n",
              "      <td>Rajshahi</td>\n",
              "      <td>RAJSHAHI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1375</th>\n",
              "      <td>Harpara</td>\n",
              "      <td>চট্টগ্রাম</td>\n",
              "      <td>Chittagong</td>\n",
              "      <td>CHITTAGONG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1376</th>\n",
              "      <td>Ahmmadpur</td>\n",
              "      <td>পাবনা</td>\n",
              "      <td>Pabna</td>\n",
              "      <td>PABNA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1377</th>\n",
              "      <td>Prembag</td>\n",
              "      <td>যশোর</td>\n",
              "      <td>Jessore</td>\n",
              "      <td>JESSORE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1378</th>\n",
              "      <td>Belka</td>\n",
              "      <td>গাইবান্ধা</td>\n",
              "      <td>Gaibandha</td>\n",
              "      <td>GAIBANDHA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1379</th>\n",
              "      <td>Comilla Dakshin</td>\n",
              "      <td>কুমিল্লা</td>\n",
              "      <td>Comilla</td>\n",
              "      <td>COMILLA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1380 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             LOCATION            Unnamed: 1 TRANSLATED DISTRICT     CAPITALIZED\n",
              "0            Lohagara               নড়াইল               Narail          NARAIL\n",
              "1           Hathazari            চট্টগ্রাম           Chittagong      CHITTAGONG\n",
              "2           Rangamati  রাঙ্গামাটি পার্বত্য       Rangamati Hill  RANGAMATI HILL\n",
              "3              Rajoir            মাদারীপুর            Madaripur       MADARIPUR\n",
              "4            Rajshahi              রাজশাহী             Rajshahi        RAJSHAHI\n",
              "...               ...                   ...                 ...             ...\n",
              "1375          Harpara            চট্টগ্রাম           Chittagong      CHITTAGONG\n",
              "1376        Ahmmadpur                পাবনা                Pabna           PABNA\n",
              "1377          Prembag                 যশোর              Jessore         JESSORE\n",
              "1378            Belka            গাইবান্ধা            Gaibandha       GAIBANDHA\n",
              "1379  Comilla Dakshin             কুমিল্লা              Comilla         COMILLA\n",
              "\n",
              "[1380 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    }
  ]
}