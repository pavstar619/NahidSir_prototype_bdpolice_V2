{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NahidSir_ACLED_Prototype.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyML/HRQDRSXeSDdjpTYP+uf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavstar619/NahidSir_prototype_bdpolice_V2/blob/main/NahidSir_ACLED_Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXrIvym_FUqv"
      },
      "source": [
        "Note: \n",
        "\n",
        "* Acled xlsx event type had some different worded rows. Fixed that after making the first total counts xlsx\n",
        "\n",
        "* (SOLVED) Tried Specific events by separate years with cities. Didnt work out due to each city having unequal amount of events\n",
        "\n",
        "  * (SOLVED) Tried using dictionary but lots of empty arrays as one city might not have any crimes at all for one year. Not sure how to append dict to excel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChDHnWza714w",
        "outputId": "a12428cb-7c45-4648-ad2e-2e87a8c09873"
      },
      "source": [
        "!pip install XlsxWriter\n",
        "!pip install scikit-multilearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: XlsxWriter in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNWsqwkyfoIJ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import folium\n",
        "\n",
        "from geopy.geocoders import Nominatim, OpenMapQuest\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_uDtTnpUAHu"
      },
      "source": [
        "# !pip install geopy\n",
        "# !pip install folium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z82cj9MeRU1",
        "outputId": "3dd6e2e2-af1f-4587-d106-e34a8e11c1ea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM8ZeogDfJas"
      },
      "source": [
        "# # data = pd.read_excel('ACLED_Data_Prototype.xlsx')\n",
        "# data = pd.read_excel('/content/gdrive/MyDrive/Thesis Mates/Thesis Part 2/ACLED Data & Paper Writing Guideline /ACLED_Data_Prototype.xlsx')\n",
        "# data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn-e-2YfYEiL"
      },
      "source": [
        "# data.loc[0, 'LOCATION']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUlt8kD-ZR6E"
      },
      "source": [
        "Specific Event types total counts by year\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ9nzSsKhWBY"
      },
      "source": [
        "# years = data.YEAR.unique()\n",
        "# events = data.EVENT_TYPE.unique() # has duplicates same values actual values is 6 not 9\n",
        "# print(events)\n",
        "\n",
        "# totalcount_df = pd.DataFrame(columns = ['YEAR'])\n",
        "\n",
        "# for i in years:\n",
        "#   year = data.loc[data['YEAR'] == i]\n",
        "#   duplicates = year.pivot_table(index = ['EVENT_TYPE'], aggfunc ='size')\n",
        "#   print(i)\n",
        "#   print(duplicates,'\\n')\n",
        "\n",
        "#   totalcount_df = totalcount_df.append(duplicates, ignore_index=True)\n",
        "\n",
        "# totalcount_df\n",
        "\n",
        "# totalcount_df.to_csv('ACLED_totalcounts.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3V4dsYFvV1x"
      },
      "source": [
        "Total Events by year and cities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu7R9LnvnhCt"
      },
      "source": [
        "# years = data['YEAR'].unique()\n",
        "# locations = data['LOCATION'].unique()\n",
        "# events = data['EVENT_TYPE'].unique()\n",
        "\n",
        "# locations_totalcount_df = pd.DataFrame(columns = [years])\n",
        "# count = 0\n",
        "\n",
        "# for i in locations:\n",
        "#   for j in years:\n",
        "#     x = data.loc[(data['LOCATION'] == i) & (data['YEAR'] == j)]\n",
        "#     # duplicates = x.pivot_table(index = ['EVENT_TYPE'], aggfunc ='size') # enable this to show specific events like riots murder etc\n",
        "#     sum = x['EVENT_TYPE'].count()\n",
        "#     # print('~~~~~~~~~~~~~~~~~~~~~~')\n",
        "#     # print(i,j,'Total events: ',sum)\n",
        "#     # print(duplicates)\n",
        "\n",
        "#     locations_totalcount_df.loc[count,j] = sum\n",
        "\n",
        "#   count+=1\n",
        "\n",
        "# # locations_totalcount_df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIqKynl0wCHJ"
      },
      "source": [
        "# # Add cities to dataframe\n",
        "# locations_totalcount_df['LOCATION'] = locations.tolist()\n",
        "# locations_totalcount_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK8WJE3tu42I"
      },
      "source": [
        "# locations_totalcount_df.to_excel('ACLED_locations_totalcount_df.xlsx',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ek_EK1BhN2-"
      },
      "source": [
        "# x = data.loc[(data['LOCATION'] == 'Hathazari') & (data['YEAR'] == 2011)]\n",
        "# x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2mXTpqfb8GE"
      },
      "source": [
        "Specific Events by years by cities (using Dictionary -> excel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLCofKy-N4pq"
      },
      "source": [
        "# years = data['YEAR'].unique()\n",
        "# locations = data['LOCATION'].unique()\n",
        "# events = data['EVENT_TYPE'].unique()\n",
        "\n",
        "# years_dict = {}\n",
        "# for i in years:\n",
        "#   location_dict = {}\n",
        "#   for j in locations:\n",
        "#     x = data.loc[(data['LOCATION'] == j) & (data['YEAR'] == i)]\n",
        "#     duplicates = x.pivot_table(index = ['EVENT_TYPE'], aggfunc ='size') # enable this to show specific events like riots murder etc\n",
        "#     event_dict = {}\n",
        "#     for event, value in duplicates.items():\n",
        "#       event_dict[event] = value \n",
        "\n",
        "#     location_dict[j] = event_dict\n",
        "\n",
        "#   years_dict[i] = location_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3OLeI-tPODg"
      },
      "source": [
        "# for i in years_dict:\n",
        "#   print(years_dict[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAI2S5FKSnZv"
      },
      "source": [
        "# writer = pd.ExcelWriter('yearwise_locations_specificevent.xlsx', engine='xlsxwriter')\n",
        "\n",
        "# yearwise_locations_specificevent = pd.DataFrame()\n",
        "# yearwise_locations_specificevent\n",
        "\n",
        "# for year in years_dict:\n",
        "#   count = 0\n",
        "#   for city in years_dict[year]:\n",
        "#     yearwise_locations_specificevent.loc[count, 'LOCATION'] = city\n",
        "\n",
        "#     x = years_dict[year][city]\n",
        "#     for event,value in x.items():\n",
        "#       yearwise_locations_specificevent.loc[count, event] = value\n",
        "#       # yearwise_locations_specificevent[str(event)].iloc[count] = value\n",
        "\n",
        "#     count+=1\n",
        "\n",
        "#     # for event in years_dict[year][city]:\n",
        "\n",
        "\n",
        "#   yearwise_locations_specificevent.to_excel(writer, sheet_name=str(year),index=False)\n",
        "\n",
        "# writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Xzxe8CUd0i"
      },
      "source": [
        "Geo Mapping Stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYqqwRdlUdf-"
      },
      "source": [
        "# plt.figure(figsize = (15,8))\n",
        "# sns.scatterplot(data['LATITUDE'], data['LONGITUDE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChlAL6--WheF"
      },
      "source": [
        "# lat_median = data['LATITUDE'].median() # y\n",
        "# long_median = data['LONGITUDE'].median() # x\n",
        "# print(lat_median,long_median)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HehUVH2sVhi8"
      },
      "source": [
        "# # create the map.\n",
        "# map_pickup = folium.Map(location=[lat_median, long_median], zoom_start = 12)\n",
        "\n",
        "# # adding the latitude and longitude points to the map.\n",
        "# data.apply(lambda row:folium.CircleMarker(location=[row['LATITUDE'], row['LONGITUDE']],).add_to(map_pickup), axis=1)\n",
        "\n",
        "# # optional: save the map.\n",
        "# map_pickup.save('geomap_bd.html')\n",
        "\n",
        "# # display the map: just ask for the object representation in juypter notebook.\n",
        "# map_pickup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk4jh_Ob0fSJ"
      },
      "source": [
        "Geo Locater find districts and stuff\n",
        "\n",
        "Problems:\n",
        " * Takes too long for each of the 30000 rows (2rows/sec speed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s83LL7IjvtBp"
      },
      "source": [
        "# # create the locator\n",
        "# geolocator = Nominatim(user_agent=\"GGWP\") # 120 req per min\n",
        "# # geolocator = OpenMapQuest(api_key='x5LPBPjUCj9CUEl4U9fUrnoLGfGv7SWk') # 275 req per min NOT WORKING as limit = 15k\n",
        "\n",
        "# # getting the location address\n",
        "# # location = geolocator.reverse(\"23.1998, 89.6644\")\n",
        "# location = geolocator.reverse((22.6432,\t92.1919))\n",
        "# print(location.address)\n",
        "# # print(location.raw['address']['state_district'])\n",
        "# print(location.raw)\n",
        "# print(geolocator.geocode('Hathazari'))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "162zpaCzBZNW"
      },
      "source": [
        "\n",
        "# from geopy.geocoders import OpenMapQuest\n",
        "\n",
        "# point = '23.1998, 89.6644' #here's famous Sherlock Holmes' museum lat & lng\n",
        "\n",
        "# geolocator = OpenMapQuest(api_key='x5LPBPjUCj9CUEl4U9fUrnoLGfGv7SWk')\n",
        "# address = geolocator.reverse(point)\n",
        "# print(address[0]) # use other indexes if you want more or less detailed address scope."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUqU8UteBlE0"
      },
      "source": [
        "# # https://stackoverflow.com/questions/31506272/geopy-too-slow-timeout-all-the-time\n",
        "# # TIME OUT ERROR FIX\n",
        "\n",
        "# def geocode(i, recursion=0):\n",
        "#     try:\n",
        "#         return geolocator.reverse((data.loc[i, 'LATITUDE'], data.loc[i, 'LONGITUDE']))\n",
        "#     except GeocoderTimedOut as e:\n",
        "#         if recursion > 10:      # max recursions\n",
        "#             raise e\n",
        "\n",
        "#         time.sleep(2) # wait a bit\n",
        "#         # try again\n",
        "#         return geocode(i, recursion=recursion + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ2Ix1Kp3zul"
      },
      "source": [
        "# # THIS ONE TAKES TOO LONG FOR 30k ROWS\n",
        "\n",
        "# for i in range(len(data.index)):\n",
        "#   # location = geolocator.reverse((data.loc[i, 'LATITUDE'], data.loc[i, 'LONGITUDE']))\n",
        "#   location = geocode(i)\n",
        "#   # print(location.raw)\n",
        "#   try:\n",
        "#     data.loc[i, 'COUNTY'] = location.raw['address']['state_district']\n",
        "#     data.loc[i, 'DIVISION'] = location.raw['address']['state']\n",
        "#   except KeyError:\n",
        "#     print('not found')\n",
        "#   print(i)\n",
        "\n",
        "# data.to_excel('location_by_State.xlsx',index=False)\n",
        "# data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL0epL5mI2MO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1aPKvfOOZyg"
      },
      "source": [
        "# RUN FOREVER WITHOUT DISCONNECT \n",
        "# while True:pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSqgH3F4sI-o"
      },
      "source": [
        "Classification Note:\n",
        " * Predict using Year,City - Find coords and event type(idk how)\n",
        " * Predict using Year, City - Find only event type (multiple events in one city how to handle?) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf31jjEcd2-N"
      },
      "source": [
        "1. Classify years with events only\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-WXIHVPgpYV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import linear_model \n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqX6QNXd2q_"
      },
      "source": [
        "# data = pd.read_excel('/content/gdrive/MyDrive/Thesis Mates/Thesis Part 2/ACLED Data & Paper Writing Guideline /Data Outputs/ACLED_event_totalcounts.xlsx')\n",
        "# data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "552OZvsSfhFC"
      },
      "source": [
        "# def R2_calc(pred_y, test_y): # Closer to 1 better score\n",
        "#   return r2_score(pred_y, test_y)\n",
        "\n",
        "# def MSE_calc(pred_y, test_y): # Lower the better\n",
        "#   return mean_squared_error(pred_y, test_y, squared = False)\n",
        "  \n",
        "# def MAE_calc(pred_y, test_y): # Lower the better\n",
        "#   return mean_absolute_error(pred_y, test_y)\n",
        "\n",
        "# def Error_calc(pred_y, test_y):\n",
        "#   return (pred_y - test_y) / test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4TkXcc0fUJw"
      },
      "source": [
        "# def modelizeall(dmp, dmp_2018):\n",
        "#   # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "#   writer = pd.ExcelWriter('ACLED_event_totalcounts_test_pred_data.xlsx', engine='xlsxwriter')\n",
        "#   # writer1 = pd.ExcelWriter('Without2018train_errors_data.xlsx', engine='xlsxwriter')\n",
        "\n",
        "#   model = Sequential()\n",
        "#   model.add(Dense(8, input_dim=1, activation='relu'))\n",
        "#   model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
        "#   model.add(Dense(1, kernel_initializer='normal'))\n",
        "#   # used binary loss function \n",
        "#   model.compile(loss='mse', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "#   # print(model.summary())\n",
        "\n",
        "#   # dmp_x = dmp.loc[:, dmp.columns != 'Robbery']\n",
        "#   # dmp_y = dmp['Robbery']\n",
        "#   for col in dmp.columns[1:].unique():\n",
        "#     print('~~~~~~~~~~~~~~~~~~~~~', col, '~~~~~~~~~~~~~~~~~~~~~')\n",
        "#     dmp_x = dmp['YEAR'].values\n",
        "#     dmp_y = dmp[col].values\n",
        "\n",
        "#     # ASSIGN MANUALLY\n",
        "#     train_x = dmp_x\n",
        "#     train_y = dmp_y\n",
        "\n",
        "#     test_x = dmp_2018['YEAR'].values\n",
        "#     test_y = dmp_2018[col].values\n",
        "\n",
        "\n",
        "#     # train_x, test_x, train_y, test_y = train_test_split(dmp_x, dmp_y, test_size = 0.25, random_state=0)\n",
        "\n",
        "#     # Standard scaling\n",
        "#     ss = StandardScaler()\n",
        "#     ss.fit(train_x.reshape(-1, 1))\n",
        "#     ss_train_x = ss.transform(train_x.reshape(-1, 1))\n",
        "#     ss_test_x = ss.transform(test_x.reshape(-1, 1))\n",
        "\n",
        "#     # print(train_x)\n",
        "#     # print(test_x)\n",
        "\n",
        "\n",
        "\n",
        "#     # pred for single data\n",
        "#     Xnew = [[2021]]\n",
        "#     # print(len(Xnew[0]))\n",
        "#     # print(len(dmp_x.columns))\n",
        "\n",
        "#     regressor = DecisionTreeRegressor(random_state=0)\n",
        "#     forest = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "#     svr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
        "#     mlp = MLPClassifier(random_state=0, max_iter = 2000)\n",
        "#     lasso = linear_model.Lasso(alpha=0.1)\n",
        "#     bayesian = linear_model.BayesianRidge()\n",
        "#     ridge = linear_model.Ridge()\n",
        "#     lr = LinearRegression()\n",
        "\n",
        "#     regressor.fit(train_x.reshape(-1, 1),train_y)  \n",
        "#     forest.fit(train_x.reshape(-1, 1),train_y) \n",
        "#     svr.fit(train_x.reshape(-1, 1),train_y)\n",
        "#     mlp.fit(ss_train_x.reshape(-1,1),train_y) \n",
        "#     lasso.fit(train_x.reshape(-1,1),train_y) \n",
        "#     bayesian.fit(train_x.reshape(-1,1),train_y) \n",
        "#     ridge.fit(train_x.reshape(-1,1),train_y) \n",
        "#     lr.fit(train_x.reshape(-1,1),train_y)\n",
        "#     model.fit(train_x,train_y, epochs=30, batch_size=2)\n",
        "\n",
        "\n",
        "\n",
        "#     pred_y = regressor.predict(test_x.reshape(-1, 1))\n",
        "#     pred_y1 = forest.predict(test_x.reshape(-1, 1)) \n",
        "#     pred_y2 = svr.predict(test_x.reshape(-1, 1)) \n",
        "#     pred_y3 = mlp.predict(ss_test_x.reshape(-1, 1))\n",
        "#     pred_y4 = lasso.predict(test_x.reshape(-1,1))\n",
        "#     pred_y5 = bayesian.predict(test_x.reshape(-1,1))\n",
        "#     pred_y6 = ridge.predict(test_x.reshape(-1,1))\n",
        "#     pred_y7 = lr.predict(test_x.reshape(-1,1))\n",
        "#     pred_y8 = model.predict(test_x.reshape(-1,1)).tolist()\n",
        "\n",
        "#     # error_y = (pred_y - test_y) / test_y #lower the better\n",
        "#     # error_y1 = (pred_y1 - test_y) / test_y\n",
        "#     # error_y2 = (pred_y2 - test_y) / test_y\n",
        "#     # error_y3 = (pred_y3 - test_y) / test_y\n",
        "#     # error_y4 = (pred_y4 - test_y) / test_y\n",
        "#     # error_y5 = (pred_y5 - test_y) / test_y\n",
        "#     # error_y6 = (pred_y6 - test_y) / test_y\n",
        "#     # error_y7 = (pred_y7 - test_y) / test_y\n",
        "\n",
        "#     pred_y_list = [pred_y,pred_y1,pred_y2,pred_y3,pred_y4,pred_y5,pred_y6,pred_y7,pred_y8]\n",
        "#     error_y_list = []\n",
        "#     r2_y_list = []\n",
        "#     mse_y_list = []\n",
        "#     mae_y_list = []\n",
        "\n",
        "#     for pred_y in pred_y_list:\n",
        "#       # error_y = Error_calc(pred_y,test_y)\n",
        "#       # error_y_list.append(error_y)\n",
        "\n",
        "#       r2_y = R2_calc(pred_y,test_y)\n",
        "#       r2_y_list.append(r2_y)\n",
        "\n",
        "#       mse_y = MSE_calc(pred_y,test_y)\n",
        "#       mse_y_list.append(mse_y)\n",
        "\n",
        "#       mae_y = MAE_calc(pred_y,test_y)\n",
        "#       mae_y_list.append(mae_y)\n",
        "\n",
        "#     Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "#     model_list = ['Decision Tree', 'Random Forest', 'SVR', 'MLP(Adam)', 'Lasso', 'Bayesian', 'Ridge', 'Linear Regression','Neural Net']\n",
        "#     # print('~~~~ Test Val || Predicted Val || Errors: R2, RMSE, MAE ~~~~')\n",
        "#     # for i in range(len(pred_y_list)):\n",
        "#     #   print(model_list[i], 'Test Val: ',test_y, 'Predicted Val: ', pred_y_list[i], 'Errors R2 | RMSE | MAE: ', r2_y_list[i], mse_y_list[i], mae_y_list[i]) # Dont show Metric as only one test row\n",
        "\n",
        "\n",
        "#     # # Put in dataframes\n",
        "#     # errors_data = pd.DataFrame(list(zip(model_list, r2_y_list, mse_y_list, mae_y_list)),\n",
        "#     #            columns =['Algorithms','R2 Score','Mean Square Error(MSE)','Mean Absolute Error(MAE)'])\n",
        "#     # # print(errors_data)\n",
        "#     test_pred_data = pd.DataFrame(columns =['Algorithms','Test Years','Test Values','Predicted Values','R2 Score','Mean Square Error(MSE)','Mean Absolute Error(MAE)'])\n",
        "#     for i in range(len(pred_y_list)):\n",
        "#       test_pred_data.loc[i] = [model_list[i], test_x, test_y, list(np.around(pred_y_list[i],2)), r2_y_list[i], mse_y_list[i], mae_y_list[i]]\n",
        "#     # print(test_pred_data)\n",
        "\n",
        "\n",
        "#     # Put in excel sheets\n",
        "#     # Write each dataframe to a different worksheet.\n",
        "#     test_pred_data.to_excel(writer, sheet_name=col,index=False)\n",
        "#     # errors_data.to_excel(writer1, sheet_name=col,index=False)\n",
        "\n",
        "\n",
        "#     # Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "\n",
        "#     # m2_y = mean_squared_error(test_y,pred_y, squared=False)\n",
        "\n",
        "#     # print('~~~~ Test Val || Predicted Val || Errors: R2, RMSE, RSE ~~~~')\n",
        "#     # # print(test_y)\n",
        "#     # print('Test DT: ',test_y,pred_y,error_y) # Decision Tree\n",
        "#     # print('Test RF: ',test_y,pred_y1,error_y1) # Random Forest\n",
        "#     # print('Test SVR: ',test_y,pred_y2,error_y2) # SVM\n",
        "#     # print('Test MLP: ',test_y,pred_y3,error_y3) # mlp\n",
        "#     # print('Test Lasso: ',test_y,pred_y4,error_y4) # lasso\n",
        "#     # print('Test Bayesian: ',test_y,pred_y5,error_y5) # bayesian\n",
        "#     # print('Test Ridge: ',test_y,pred_y6,error_y6) # ridge\n",
        "#     # print('Test Linear Regression: ',test_y,pred_y7,error_y7) # lr\n",
        "\n",
        "#     # print('2018 BEST MLP: ',test_y,pred_y3) # Decision Tree\n",
        "#     print(Xnew,'BEST MLP: ',col,Xpred_y) # Decision Tree\n",
        "\n",
        "\n",
        "#   # Close the Pandas Excel writer and output the Excel file.\n",
        "#   writer.save()\n",
        "#   # writer1.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRZZnCMlfNFr"
      },
      "source": [
        "# # 2018 dmp\n",
        "# dmp_2018 = data.iloc[19:] # 2020\n",
        "# print(dmp_2018)\n",
        "\n",
        "# # 2010 to 2017\n",
        "# dmp_till2017 = data.drop(19)\n",
        "# print(dmp_till2017)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU46Yow0j-WJ"
      },
      "source": [
        "# modelizeall(dmp_till2017, dmp_2018)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz5xZ01U3DQr"
      },
      "source": [
        "1.1 Predict 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEDWRGeakpHA"
      },
      "source": [
        "# dmp = data\n",
        "# # MODELIZE 2021\n",
        "# for col in data.columns[1:].unique():\n",
        "#   print('~~~~~~~~~~~~~~~~~~~~~', col, '~~~~~~~~~~~~~~~~~~~~~')\n",
        "#   dmp_x = dmp['YEAR'].values\n",
        "#   dmp_y = dmp[col].values\n",
        "\n",
        "#   # ASSIGN MANUALLY\n",
        "#   train_x = dmp_x\n",
        "#   train_y = dmp_y\n",
        "\n",
        "#   # Standard scaling\n",
        "#   ss = StandardScaler()\n",
        "#   ss.fit(train_x.reshape(-1, 1))\n",
        "#   ss_train_x = ss.transform(train_x.reshape(-1, 1))\n",
        "\n",
        "\n",
        "#   # pred for single data\n",
        "#   Xnew = [[2021]]\n",
        "#   # print(len(Xnew[0]))\n",
        "#   # print(len(dmp_x.columns))\n",
        "\n",
        "#   mlp = MLPClassifier(random_state=0, max_iter = 5000)\n",
        "#   mlp.fit(ss_train_x.reshape(-1,1),train_y) \n",
        "\n",
        "#   Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "#   print(Xnew,'BEST MLP: ',col,Xpred_y) # Decision Tree\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM0KUqN528Xf"
      },
      "source": [
        "2. Multi Label Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD2ZupaKtTIP"
      },
      "source": [
        "Problem:\n",
        " * Low precisions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXXnaYbjL2Ud"
      },
      "source": [
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "m-nxHHDB3KxR",
        "outputId": "70937da3-3006-4a8d-f1a3-b25f40cf4ac8"
      },
      "source": [
        "data = pd.read_excel('/content/gdrive/MyDrive/Thesis Mates/Thesis Part 2/ACLED Data & Paper Writing Guideline /ACLED_Data_Prototype.xlsx')\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>YEAR</th>\n",
              "      <th>EVENT_TYPE</th>\n",
              "      <th>LOCATION</th>\n",
              "      <th>LATITUDE</th>\n",
              "      <th>LONGITUDE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>Battles</td>\n",
              "      <td>Lohagara</td>\n",
              "      <td>23.1998</td>\n",
              "      <td>89.6644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Hathazari</td>\n",
              "      <td>22.5052</td>\n",
              "      <td>91.8134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Rangamati</td>\n",
              "      <td>22.6432</td>\n",
              "      <td>92.1919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Rajoir</td>\n",
              "      <td>23.1878</td>\n",
              "      <td>90.0322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Rajshahi</td>\n",
              "      <td>24.3740</td>\n",
              "      <td>88.6011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30639</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Paba</td>\n",
              "      <td>24.4417</td>\n",
              "      <td>88.6278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30640</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Bhuapur</td>\n",
              "      <td>24.4604</td>\n",
              "      <td>89.8727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30641</th>\n",
              "      <td>2020</td>\n",
              "      <td>Protests</td>\n",
              "      <td>Dhaka</td>\n",
              "      <td>23.7406</td>\n",
              "      <td>90.3943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30642</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Bogra</td>\n",
              "      <td>24.8510</td>\n",
              "      <td>89.3711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30643</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Dhaka</td>\n",
              "      <td>23.7333</td>\n",
              "      <td>90.4000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30644 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       YEAR EVENT_TYPE   LOCATION  LATITUDE  LONGITUDE\n",
              "0      2001    Battles   Lohagara   23.1998    89.6644\n",
              "1      2001      Riots  Hathazari   22.5052    91.8134\n",
              "2      2001      Riots  Rangamati   22.6432    92.1919\n",
              "3      2001      Riots     Rajoir   23.1878    90.0322\n",
              "4      2001      Riots   Rajshahi   24.3740    88.6011\n",
              "...     ...        ...        ...       ...        ...\n",
              "30639  2020      Riots       Paba   24.4417    88.6278\n",
              "30640  2020      Riots    Bhuapur   24.4604    89.8727\n",
              "30641  2020   Protests     Dhaka    23.7406    90.3943\n",
              "30642  2020      Riots      Bogra   24.8510    89.3711\n",
              "30643  2020      Riots     Dhaka    23.7333    90.4000\n",
              "\n",
              "[30644 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2542
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7_hwtDM3rLU"
      },
      "source": [
        "# x = data.copy()\n",
        "# x = x.drop('YEAR',axis=1)\n",
        "# x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_crY-gUTL6LN",
        "outputId": "ac3837e9-d44a-4050-c6dd-2d9081923299"
      },
      "source": [
        "x = data\n",
        "x = x.drop('EVENT_TYPE', axis = 1)\n",
        "y = data['EVENT_TYPE']\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       YEAR   LOCATION  LATITUDE  LONGITUDE\n",
            "0      2001   Lohagara   23.1998    89.6644\n",
            "1      2001  Hathazari   22.5052    91.8134\n",
            "2      2001  Rangamati   22.6432    92.1919\n",
            "3      2001     Rajoir   23.1878    90.0322\n",
            "4      2001   Rajshahi   24.3740    88.6011\n",
            "...     ...        ...       ...        ...\n",
            "30639  2020       Paba   24.4417    88.6278\n",
            "30640  2020    Bhuapur   24.4604    89.8727\n",
            "30641  2020     Dhaka    23.7406    90.3943\n",
            "30642  2020      Bogra   24.8510    89.3711\n",
            "30643  2020     Dhaka    23.7333    90.4000\n",
            "\n",
            "[30644 rows x 4 columns]\n",
            "0         Battles\n",
            "1           Riots\n",
            "2           Riots\n",
            "3           Riots\n",
            "4           Riots\n",
            "           ...   \n",
            "30639       Riots\n",
            "30640       Riots\n",
            "30641    Protests\n",
            "30642       Riots\n",
            "30643       Riots\n",
            "Name: EVENT_TYPE, Length: 30644, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca56odIXlnqK",
        "outputId": "d91d9982-f963-48a2-d30d-e3443ff214e5"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y)\n",
        "y = le.transform(y)\n",
        "print(le.classes_)\n",
        "# print(y)\n",
        "\n",
        "le.fit(x['LOCATION'])\n",
        "x['LOCATION'] = le.transform(x['LOCATION'])\n",
        "# print(x['LOCATION'])\n",
        "\n",
        "ss = StandardScaler()\n",
        "ss.fit(x)\n",
        "x = ss.transform(x)\n",
        "# print(x)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "models = [('DT', DecisionTreeClassifier()),\n",
        "          ('SVM', make_pipeline(StandardScaler(), SVC(gamma='auto'))),\n",
        "          ('KNN', KNeighborsClassifier()),\n",
        "          ('Naive Bayes',GaussianNB()),\n",
        "          ('Logistics Regression', LogisticRegression(random_state=0))]\n",
        "\n",
        "for name, model in models:\n",
        "  clf = model.fit(X_train, y_train)\n",
        "  predictions = clf.predict(X_test)\n",
        "\n",
        "  from sklearn.metrics import classification_report\n",
        "  from sklearn.metrics import precision_recall_fscore_support\n",
        "  \n",
        "  print(name)\n",
        "  print('Precision | Recall | F1 Score | Support')\n",
        "  print(precision_recall_fscore_support(y_test, predictions, average='weighted', labels=np.unique(predictions)))\n",
        "  # print(classification_report(y_test,predictions, target_names=(le.classes_).tolist()))\n",
        "  # print(classification_report(y_test,predictions, target_names=(le.classes_).tolist()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Battles' 'ExplosionsRemoteViolence' 'Protests' 'Riots'\n",
            " 'StrategicDevelopments' 'ViolenceAgainstCivilians']\n",
            "DT\n",
            "Precision | Recall | F1 Score | Support\n",
            "(0.5063101466032426, 0.5260409868163425, 0.5112358769284301, None)\n",
            "SVM\n",
            "Precision | Recall | F1 Score | Support\n",
            "(0.5100105760667655, 0.5726774336938806, 0.47270693595721314, None)\n",
            "KNN\n",
            "Precision | Recall | F1 Score | Support\n",
            "(0.4822244475623318, 0.5038506722360006, 0.48635947170832305, None)\n",
            "Naive Bayes\n",
            "Precision | Recall | F1 Score | Support\n",
            "(0.4497010922705968, 0.5298562750037042, 0.42481083251208995, None)\n",
            "Logistics Regression\n",
            "Precision | Recall | F1 Score | Support\n",
            "(0.5000984672975346, 0.6544474393530997, 0.4989765063368531, None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr5L05wkZ7ip"
      },
      "source": [
        "Problem:\n",
        " * Only 5 labels shown but should be 6 events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fou5Oiq0Nvey"
      },
      "source": [
        "# data['EVENT_TYPE'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QskWuIlJM20w"
      },
      "source": [
        "# # x, y = make_multilabel_classification(n_samples = len(data['EVENT_TYPE'].unique()), random_state=0,return_indicator=False)\n",
        "# # this will generate a random multi-label dataset\n",
        "# x, y = make_multilabel_classification(sparse = True, n_labels = 6, return_indicator = 'sparse', allow_unlabeled = False)\n",
        "# # print(x)\n",
        "# # print(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awc95A1YPT_J"
      },
      "source": [
        "# # using binary relevance\n",
        "# from skmultilearn.problem_transform import BinaryRelevance\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# # initialize binary relevance multi-label classifier\n",
        "# # with a gaussian naive bayes base classifier\n",
        "# classifier = BinaryRelevance(GaussianNB())\n",
        "\n",
        "# # train\n",
        "# classifier.fit(X_train, y_train)\n",
        "\n",
        "# # predict\n",
        "# predictions = classifier.predict(X_test)\n",
        "# print(predictions.toarray())\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "# print(classification_report(y_test.toarray(),predictions))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}