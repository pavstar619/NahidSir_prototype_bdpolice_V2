{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NahidSir_ACLED_Prototype.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbtD2Fb7JNip39aDZEN2P8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavstar619/NahidSir_prototype_bdpolice_V2/blob/main/NahidSir_ACLED_Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXrIvym_FUqv"
      },
      "source": [
        "Note: \n",
        "\n",
        "* Acled xlsx event type had some different worded rows. Fixed that after making the first total counts xlsx\n",
        "\n",
        "* (SOLVED) Tried Specific events by separate years with cities. Didnt work out due to each city having unequal amount of events\n",
        "\n",
        "  * (SOLVED) Tried using dictionary but lots of empty arrays as one city might not have any crimes at all for one year. Not sure how to append dict to excel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChDHnWza714w",
        "outputId": "32b8a339-b9da-40fb-fb6b-ed79bb040345"
      },
      "source": [
        "!pip install XlsxWriter\n",
        "!pip install scikit-multilearn"
      ],
      "execution_count": 3372,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: XlsxWriter in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNWsqwkyfoIJ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import folium\n",
        "\n",
        "from geopy.geocoders import Nominatim, OpenMapQuest\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "\n",
        "\n"
      ],
      "execution_count": 3373,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_uDtTnpUAHu"
      },
      "source": [
        "# !pip install geopy\n",
        "# !pip install folium"
      ],
      "execution_count": 3374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z82cj9MeRU1",
        "outputId": "94a8c5ab-cddb-48aa-f020-8d7619f00367"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 3375,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM8ZeogDfJas"
      },
      "source": [
        "# # data = pd.read_excel('ACLED_Data_Prototype.xlsx')\n",
        "# data = pd.read_excel('/content/gdrive/MyDrive/Thesis Mates/Thesis Part 2/ACLED Data & Paper Writing Guideline /ACLED_Data_Prototype.xlsx')\n",
        "# data"
      ],
      "execution_count": 3376,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn-e-2YfYEiL"
      },
      "source": [
        "# data.loc[0, 'LOCATION']"
      ],
      "execution_count": 3377,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUlt8kD-ZR6E"
      },
      "source": [
        "Specific Event types total counts by year\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ9nzSsKhWBY"
      },
      "source": [
        "# years = data.YEAR.unique()\n",
        "# events = data.EVENT_TYPE.unique() # has duplicates same values actual values is 6 not 9\n",
        "# print(events)\n",
        "\n",
        "# totalcount_df = pd.DataFrame(columns = ['YEAR'])\n",
        "\n",
        "# for i in years:\n",
        "#   year = data.loc[data['YEAR'] == i]\n",
        "#   duplicates = year.pivot_table(index = ['EVENT_TYPE'], aggfunc ='size')\n",
        "#   print(i)\n",
        "#   print(duplicates,'\\n')\n",
        "\n",
        "#   totalcount_df = totalcount_df.append(duplicates, ignore_index=True)\n",
        "\n",
        "# totalcount_df\n",
        "\n",
        "# totalcount_df.to_csv('ACLED_totalcounts.csv',index=False)"
      ],
      "execution_count": 3378,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3V4dsYFvV1x"
      },
      "source": [
        "Total Events by year and cities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu7R9LnvnhCt"
      },
      "source": [
        "# years = data['YEAR'].unique()\n",
        "# locations = data['LOCATION'].unique()\n",
        "# events = data['EVENT_TYPE'].unique()\n",
        "\n",
        "# locations_totalcount_df = pd.DataFrame(columns = [years])\n",
        "# count = 0\n",
        "\n",
        "# for i in locations:\n",
        "#   for j in years:\n",
        "#     x = data.loc[(data['LOCATION'] == i) & (data['YEAR'] == j)]\n",
        "#     # duplicates = x.pivot_table(index = ['EVENT_TYPE'], aggfunc ='size') # enable this to show specific events like riots murder etc\n",
        "#     sum = x['EVENT_TYPE'].count()\n",
        "#     # print('~~~~~~~~~~~~~~~~~~~~~~')\n",
        "#     # print(i,j,'Total events: ',sum)\n",
        "#     # print(duplicates)\n",
        "\n",
        "#     locations_totalcount_df.loc[count,j] = sum\n",
        "\n",
        "#   count+=1\n",
        "\n",
        "# # locations_totalcount_df\n",
        "\n"
      ],
      "execution_count": 3379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIqKynl0wCHJ"
      },
      "source": [
        "# # Add cities to dataframe\n",
        "# locations_totalcount_df['LOCATION'] = locations.tolist()\n",
        "# locations_totalcount_df"
      ],
      "execution_count": 3380,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK8WJE3tu42I"
      },
      "source": [
        "# locations_totalcount_df.to_excel('ACLED_locations_totalcount_df.xlsx',index=False)"
      ],
      "execution_count": 3381,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ek_EK1BhN2-"
      },
      "source": [
        "# x = data.loc[(data['LOCATION'] == 'Hathazari') & (data['YEAR'] == 2011)]\n",
        "# x"
      ],
      "execution_count": 3382,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2mXTpqfb8GE"
      },
      "source": [
        "Specific Events by years by cities (using Dictionary -> excel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLCofKy-N4pq"
      },
      "source": [
        "# years = data['YEAR'].unique()\n",
        "# locations = data['LOCATION'].unique()\n",
        "# events = data['EVENT_TYPE'].unique()\n",
        "\n",
        "# years_dict = {}\n",
        "# for i in years:\n",
        "#   location_dict = {}\n",
        "#   for j in locations:\n",
        "#     x = data.loc[(data['LOCATION'] == j) & (data['YEAR'] == i)]\n",
        "#     duplicates = x.pivot_table(index = ['EVENT_TYPE'], aggfunc ='size') # enable this to show specific events like riots murder etc\n",
        "#     event_dict = {}\n",
        "#     for event, value in duplicates.items():\n",
        "#       event_dict[event] = value \n",
        "\n",
        "#     location_dict[j] = event_dict\n",
        "\n",
        "#   years_dict[i] = location_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3383,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3OLeI-tPODg"
      },
      "source": [
        "# for i in years_dict:\n",
        "#   print(years_dict[i])"
      ],
      "execution_count": 3384,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAI2S5FKSnZv"
      },
      "source": [
        "# writer = pd.ExcelWriter('yearwise_locations_specificevent.xlsx', engine='xlsxwriter')\n",
        "\n",
        "# yearwise_locations_specificevent = pd.DataFrame()\n",
        "# yearwise_locations_specificevent\n",
        "\n",
        "# for year in years_dict:\n",
        "#   count = 0\n",
        "#   for city in years_dict[year]:\n",
        "#     yearwise_locations_specificevent.loc[count, 'LOCATION'] = city\n",
        "\n",
        "#     x = years_dict[year][city]\n",
        "#     for event,value in x.items():\n",
        "#       yearwise_locations_specificevent.loc[count, event] = value\n",
        "#       # yearwise_locations_specificevent[str(event)].iloc[count] = value\n",
        "\n",
        "#     count+=1\n",
        "\n",
        "#     # for event in years_dict[year][city]:\n",
        "\n",
        "\n",
        "#   yearwise_locations_specificevent.to_excel(writer, sheet_name=str(year),index=False)\n",
        "\n",
        "# writer.save()"
      ],
      "execution_count": 3385,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Xzxe8CUd0i"
      },
      "source": [
        "Geo Mapping Stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYqqwRdlUdf-"
      },
      "source": [
        "# plt.figure(figsize = (15,8))\n",
        "# sns.scatterplot(data['LATITUDE'], data['LONGITUDE'])"
      ],
      "execution_count": 3386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChlAL6--WheF"
      },
      "source": [
        "# lat_median = data['LATITUDE'].median() # y\n",
        "# long_median = data['LONGITUDE'].median() # x\n",
        "# print(lat_median,long_median)"
      ],
      "execution_count": 3387,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HehUVH2sVhi8"
      },
      "source": [
        "# # create the map.\n",
        "# map_pickup = folium.Map(location=[lat_median, long_median], zoom_start = 12)\n",
        "\n",
        "# # adding the latitude and longitude points to the map.\n",
        "# data.apply(lambda row:folium.CircleMarker(location=[row['LATITUDE'], row['LONGITUDE']],).add_to(map_pickup), axis=1)\n",
        "\n",
        "# # optional: save the map.\n",
        "# map_pickup.save('geomap_bd.html')\n",
        "\n",
        "# # display the map: just ask for the object representation in juypter notebook.\n",
        "# map_pickup"
      ],
      "execution_count": 3388,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk4jh_Ob0fSJ"
      },
      "source": [
        "Geo Locater find districts and stuff\n",
        "\n",
        "Problems:\n",
        " * Takes too long for each of the 30000 rows (2rows/sec speed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s83LL7IjvtBp"
      },
      "source": [
        "# # create the locator\n",
        "# geolocator = Nominatim(user_agent=\"GGWP\") # 120 req per min\n",
        "# # geolocator = OpenMapQuest(api_key='x5LPBPjUCj9CUEl4U9fUrnoLGfGv7SWk') # 275 req per min NOT WORKING as limit = 15k\n",
        "\n",
        "# # getting the location address\n",
        "# # location = geolocator.reverse(\"23.1998, 89.6644\")\n",
        "# location = geolocator.reverse((22.6432,\t92.1919))\n",
        "# print(location.address)\n",
        "# # print(location.raw['address']['state_district'])\n",
        "# print(location.raw)\n",
        "# print(geolocator.geocode('Hathazari'))\n",
        "\n",
        "\n"
      ],
      "execution_count": 3389,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "162zpaCzBZNW"
      },
      "source": [
        "\n",
        "# from geopy.geocoders import OpenMapQuest\n",
        "\n",
        "# point = '23.1998, 89.6644' #here's famous Sherlock Holmes' museum lat & lng\n",
        "\n",
        "# geolocator = OpenMapQuest(api_key='x5LPBPjUCj9CUEl4U9fUrnoLGfGv7SWk')\n",
        "# address = geolocator.reverse(point)\n",
        "# print(address[0]) # use other indexes if you want more or less detailed address scope."
      ],
      "execution_count": 3390,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUqU8UteBlE0"
      },
      "source": [
        "# # https://stackoverflow.com/questions/31506272/geopy-too-slow-timeout-all-the-time\n",
        "# # TIME OUT ERROR FIX\n",
        "\n",
        "# def geocode(i, recursion=0):\n",
        "#     try:\n",
        "#         return geolocator.reverse((data.loc[i, 'LATITUDE'], data.loc[i, 'LONGITUDE']))\n",
        "#     except GeocoderTimedOut as e:\n",
        "#         if recursion > 10:      # max recursions\n",
        "#             raise e\n",
        "\n",
        "#         time.sleep(2) # wait a bit\n",
        "#         # try again\n",
        "#         return geocode(i, recursion=recursion + 1)"
      ],
      "execution_count": 3391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ2Ix1Kp3zul"
      },
      "source": [
        "# # THIS ONE TAKES TOO LONG FOR 30k ROWS\n",
        "\n",
        "# for i in range(len(data.index)):\n",
        "#   # location = geolocator.reverse((data.loc[i, 'LATITUDE'], data.loc[i, 'LONGITUDE']))\n",
        "#   location = geocode(i)\n",
        "#   # print(location.raw)\n",
        "#   try:\n",
        "#     data.loc[i, 'COUNTY'] = location.raw['address']['state_district']\n",
        "#     data.loc[i, 'DIVISION'] = location.raw['address']['state']\n",
        "#   except KeyError:\n",
        "#     print('not found')\n",
        "#   print(i)\n",
        "\n",
        "# data.to_excel('location_by_State.xlsx',index=False)\n",
        "# data"
      ],
      "execution_count": 3392,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL0epL5mI2MO"
      },
      "source": [
        ""
      ],
      "execution_count": 3392,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1aPKvfOOZyg"
      },
      "source": [
        "# RUN FOREVER WITHOUT DISCONNECT \n",
        "# while True:pass"
      ],
      "execution_count": 3393,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSqgH3F4sI-o"
      },
      "source": [
        "Classification Note:\n",
        " * Predict using Year,City - Find coords and event type(idk how)\n",
        " * Predict using Year, City - Find only event type (multiple events in one city how to handle?) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf31jjEcd2-N"
      },
      "source": [
        "1. Classify years with events only\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-WXIHVPgpYV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import linear_model \n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqX6QNXd2q_"
      },
      "source": [
        "# data = pd.read_excel('/content/gdrive/MyDrive/Thesis Mates/Thesis Part 2/ACLED Data & Paper Writing Guideline /Data Outputs/ACLED_event_totalcounts.xlsx')\n",
        "# data"
      ],
      "execution_count": 3395,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "552OZvsSfhFC"
      },
      "source": [
        "# def R2_calc(pred_y, test_y): # Closer to 1 better score\n",
        "#   return r2_score(pred_y, test_y)\n",
        "\n",
        "# def MSE_calc(pred_y, test_y): # Lower the better\n",
        "#   return mean_squared_error(pred_y, test_y, squared = False)\n",
        "  \n",
        "# def MAE_calc(pred_y, test_y): # Lower the better\n",
        "#   return mean_absolute_error(pred_y, test_y)\n",
        "\n",
        "# def Error_calc(pred_y, test_y):\n",
        "#   return (pred_y - test_y) / test_y"
      ],
      "execution_count": 3396,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4TkXcc0fUJw"
      },
      "source": [
        "# def modelizeall(dmp, dmp_2018):\n",
        "#   # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "#   writer = pd.ExcelWriter('ACLED_event_totalcounts_test_pred_data.xlsx', engine='xlsxwriter')\n",
        "#   # writer1 = pd.ExcelWriter('Without2018train_errors_data.xlsx', engine='xlsxwriter')\n",
        "\n",
        "#   model = Sequential()\n",
        "#   model.add(Dense(8, input_dim=1, activation='relu'))\n",
        "#   model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
        "#   model.add(Dense(1, kernel_initializer='normal'))\n",
        "#   # used binary loss function \n",
        "#   model.compile(loss='mse', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "#   # print(model.summary())\n",
        "\n",
        "#   # dmp_x = dmp.loc[:, dmp.columns != 'Robbery']\n",
        "#   # dmp_y = dmp['Robbery']\n",
        "#   for col in dmp.columns[1:].unique():\n",
        "#     print('~~~~~~~~~~~~~~~~~~~~~', col, '~~~~~~~~~~~~~~~~~~~~~')\n",
        "#     dmp_x = dmp['YEAR'].values\n",
        "#     dmp_y = dmp[col].values\n",
        "\n",
        "#     # ASSIGN MANUALLY\n",
        "#     train_x = dmp_x\n",
        "#     train_y = dmp_y\n",
        "\n",
        "#     test_x = dmp_2018['YEAR'].values\n",
        "#     test_y = dmp_2018[col].values\n",
        "\n",
        "\n",
        "#     # train_x, test_x, train_y, test_y = train_test_split(dmp_x, dmp_y, test_size = 0.25, random_state=0)\n",
        "\n",
        "#     # Standard scaling\n",
        "#     ss = StandardScaler()\n",
        "#     ss.fit(train_x.reshape(-1, 1))\n",
        "#     alpha_train_x = ss.transform(train_x.reshape(-1, 1))\n",
        "#     ss_test_x = ss.transform(test_x.reshape(-1, 1))\n",
        "\n",
        "#     # print(train_x)\n",
        "#     # print(test_x)\n",
        "\n",
        "\n",
        "\n",
        "#     # pred for single data\n",
        "#     Xnew = [[2021]]\n",
        "#     # print(len(Xnew[0]))\n",
        "#     # print(len(dmp_x.columns))\n",
        "\n",
        "#     regressor = DecisionTreeRegressor(random_state=0)\n",
        "#     forest = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "#     svr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
        "#     mlp = MLPClassifier(random_state=0, max_iter = 2000)\n",
        "#     lasso = linear_model.Lasso(alpha=0.1)\n",
        "#     bayesian = linear_model.BayesianRidge()\n",
        "#     ridge = linear_model.Ridge()\n",
        "#     lr = LinearRegression()\n",
        "\n",
        "#     regressor.fit(train_x.reshape(-1, 1),train_y)  \n",
        "#     forest.fit(train_x.reshape(-1, 1),train_y) \n",
        "#     svr.fit(train_x.reshape(-1, 1),train_y)\n",
        "#     mlp.fit(alpha_train_x.reshape(-1,1),train_y) \n",
        "#     lasso.fit(train_x.reshape(-1,1),train_y) \n",
        "#     bayesian.fit(train_x.reshape(-1,1),train_y) \n",
        "#     ridge.fit(train_x.reshape(-1,1),train_y) \n",
        "#     lr.fit(train_x.reshape(-1,1),train_y)\n",
        "#     model.fit(train_x,train_y, epochs=30, batch_size=2)\n",
        "\n",
        "\n",
        "\n",
        "#     pred_y = regressor.predict(test_x.reshape(-1, 1))\n",
        "#     pred_y1 = forest.predict(test_x.reshape(-1, 1)) \n",
        "#     pred_y2 = svr.predict(test_x.reshape(-1, 1)) \n",
        "#     pred_y3 = mlp.predict(ss_test_x.reshape(-1, 1))\n",
        "#     pred_y4 = lasso.predict(test_x.reshape(-1,1))\n",
        "#     pred_y5 = bayesian.predict(test_x.reshape(-1,1))\n",
        "#     pred_y6 = ridge.predict(test_x.reshape(-1,1))\n",
        "#     pred_y7 = lr.predict(test_x.reshape(-1,1))\n",
        "#     pred_y8 = model.predict(test_x.reshape(-1,1)).tolist()\n",
        "\n",
        "#     # error_y = (pred_y - test_y) / test_y #lower the better\n",
        "#     # error_y1 = (pred_y1 - test_y) / test_y\n",
        "#     # error_y2 = (pred_y2 - test_y) / test_y\n",
        "#     # error_y3 = (pred_y3 - test_y) / test_y\n",
        "#     # error_y4 = (pred_y4 - test_y) / test_y\n",
        "#     # error_y5 = (pred_y5 - test_y) / test_y\n",
        "#     # error_y6 = (pred_y6 - test_y) / test_y\n",
        "#     # error_y7 = (pred_y7 - test_y) / test_y\n",
        "\n",
        "#     pred_y_list = [pred_y,pred_y1,pred_y2,pred_y3,pred_y4,pred_y5,pred_y6,pred_y7,pred_y8]\n",
        "#     error_y_list = []\n",
        "#     r2_y_list = []\n",
        "#     mse_y_list = []\n",
        "#     mae_y_list = []\n",
        "\n",
        "#     for pred_y in pred_y_list:\n",
        "#       # error_y = Error_calc(pred_y,test_y)\n",
        "#       # error_y_list.append(error_y)\n",
        "\n",
        "#       r2_y = R2_calc(pred_y,test_y)\n",
        "#       r2_y_list.append(r2_y)\n",
        "\n",
        "#       mse_y = MSE_calc(pred_y,test_y)\n",
        "#       mse_y_list.append(mse_y)\n",
        "\n",
        "#       mae_y = MAE_calc(pred_y,test_y)\n",
        "#       mae_y_list.append(mae_y)\n",
        "\n",
        "#     Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "#     model_list = ['Decision Tree', 'Random Forest', 'SVR', 'MLP(Adam)', 'Lasso', 'Bayesian', 'Ridge', 'Linear Regression','Neural Net']\n",
        "#     # print('~~~~ Test Val || Predicted Val || Errors: R2, RMSE, MAE ~~~~')\n",
        "#     # for i in range(len(pred_y_list)):\n",
        "#     #   print(model_list[i], 'Test Val: ',test_y, 'Predicted Val: ', pred_y_list[i], 'Errors R2 | RMSE | MAE: ', r2_y_list[i], mse_y_list[i], mae_y_list[i]) # Dont show Metric as only one test row\n",
        "\n",
        "\n",
        "#     # # Put in dataframes\n",
        "#     # errors_data = pd.DataFrame(list(zip(model_list, r2_y_list, mse_y_list, mae_y_list)),\n",
        "#     #            columns =['Algorithms','R2 Score','Mean Square Error(MSE)','Mean Absolute Error(MAE)'])\n",
        "#     # # print(errors_data)\n",
        "#     test_pred_data = pd.DataFrame(columns =['Algorithms','Test Years','Test Values','Predicted Values','R2 Score','Mean Square Error(MSE)','Mean Absolute Error(MAE)'])\n",
        "#     for i in range(len(pred_y_list)):\n",
        "#       test_pred_data.loc[i] = [model_list[i], test_x, test_y, list(np.around(pred_y_list[i],2)), r2_y_list[i], mse_y_list[i], mae_y_list[i]]\n",
        "#     # print(test_pred_data)\n",
        "\n",
        "\n",
        "#     # Put in excel sheets\n",
        "#     # Write each dataframe to a different worksheet.\n",
        "#     test_pred_data.to_excel(writer, sheet_name=col,index=False)\n",
        "#     # errors_data.to_excel(writer1, sheet_name=col,index=False)\n",
        "\n",
        "\n",
        "#     # Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "\n",
        "#     # m2_y = mean_squared_error(test_y,pred_y, squared=False)\n",
        "\n",
        "#     # print('~~~~ Test Val || Predicted Val || Errors: R2, RMSE, RSE ~~~~')\n",
        "#     # # print(test_y)\n",
        "#     # print('Test DT: ',test_y,pred_y,error_y) # Decision Tree\n",
        "#     # print('Test RF: ',test_y,pred_y1,error_y1) # Random Forest\n",
        "#     # print('Test SVR: ',test_y,pred_y2,error_y2) # SVM\n",
        "#     # print('Test MLP: ',test_y,pred_y3,error_y3) # mlp\n",
        "#     # print('Test Lasso: ',test_y,pred_y4,error_y4) # lasso\n",
        "#     # print('Test Bayesian: ',test_y,pred_y5,error_y5) # bayesian\n",
        "#     # print('Test Ridge: ',test_y,pred_y6,error_y6) # ridge\n",
        "#     # print('Test Linear Regression: ',test_y,pred_y7,error_y7) # lr\n",
        "\n",
        "#     # print('2018 BEST MLP: ',test_y,pred_y3) # Decision Tree\n",
        "#     print(Xnew,'BEST MLP: ',col,Xpred_y) # Decision Tree\n",
        "\n",
        "\n",
        "#   # Close the Pandas Excel writer and output the Excel file.\n",
        "#   writer.save()\n",
        "#   # writer1.save()"
      ],
      "execution_count": 3397,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRZZnCMlfNFr"
      },
      "source": [
        "# # 2018 dmp\n",
        "# dmp_2018 = data.iloc[19:] # 2020\n",
        "# print(dmp_2018)\n",
        "\n",
        "# # 2010 to 2017\n",
        "# dmp_till2017 = data.drop(19)\n",
        "# print(dmp_till2017)\n"
      ],
      "execution_count": 3398,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU46Yow0j-WJ"
      },
      "source": [
        "# modelizeall(dmp_till2017, dmp_2018)"
      ],
      "execution_count": 3399,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz5xZ01U3DQr"
      },
      "source": [
        "1.1 Predict 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEDWRGeakpHA"
      },
      "source": [
        "# dmp = data\n",
        "# # MODELIZE 2021\n",
        "# for col in data.columns[1:].unique():\n",
        "#   print('~~~~~~~~~~~~~~~~~~~~~', col, '~~~~~~~~~~~~~~~~~~~~~')\n",
        "#   dmp_x = dmp['YEAR'].values\n",
        "#   dmp_y = dmp[col].values\n",
        "\n",
        "#   # ASSIGN MANUALLY\n",
        "#   train_x = dmp_x\n",
        "#   train_y = dmp_y\n",
        "\n",
        "#   # Standard scaling\n",
        "#   ss = StandardScaler()\n",
        "#   ss.fit(train_x.reshape(-1, 1))\n",
        "#   ss_train_x = ss.transform(train_x.reshape(-1, 1))\n",
        "\n",
        "\n",
        "#   # pred for single data\n",
        "#   Xnew = [[2021]]\n",
        "#   # print(len(Xnew[0]))\n",
        "#   # print(len(dmp_x.columns))\n",
        "\n",
        "#   mlp = MLPClassifier(random_state=0, max_iter = 5000)\n",
        "#   mlp.fit(ss_train_x.reshape(-1,1),train_y) \n",
        "\n",
        "#   Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "#   print(Xnew,'BEST MLP: ',col,Xpred_y) # Decision Tree\n",
        "\n",
        "\n"
      ],
      "execution_count": 3400,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM0KUqN528Xf"
      },
      "source": [
        "2. Multi Label Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD2ZupaKtTIP"
      },
      "source": [
        "Did: \n",
        "* Predicted Event \n",
        "* Predicted Lat,long \n",
        "* Predicted Location\n",
        "\n",
        "Left: \n",
        "* Predict Lat, Long, Event\n",
        "\n",
        "Problem:\n",
        "* Low precisions\n",
        "\n",
        "BREAKTHROUGH:\n",
        "* IMPROVED MSE from 0.95 to 0.18 by scaling the y data. R2 from 0.2 to 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXXnaYbjL2Ud"
      },
      "source": [
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ],
      "execution_count": 3401,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "m-nxHHDB3KxR",
        "outputId": "b652d4ee-d446-4ff6-9e20-47babec442f6"
      },
      "source": [
        "data = pd.read_excel('/content/gdrive/MyDrive/Thesis Mates/Thesis Part 2/ACLED Data & Paper Writing Guideline /ACLED_Data_Prototype.xlsx')\n",
        "data"
      ],
      "execution_count": 3402,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>YEAR</th>\n",
              "      <th>EVENT_TYPE</th>\n",
              "      <th>LOCATION</th>\n",
              "      <th>LATITUDE</th>\n",
              "      <th>LONGITUDE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>Battles</td>\n",
              "      <td>Lohagara</td>\n",
              "      <td>23.1998</td>\n",
              "      <td>89.6644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Hathazari</td>\n",
              "      <td>22.5052</td>\n",
              "      <td>91.8134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Rangamati</td>\n",
              "      <td>22.6432</td>\n",
              "      <td>92.1919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Rajoir</td>\n",
              "      <td>23.1878</td>\n",
              "      <td>90.0322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Rajshahi</td>\n",
              "      <td>24.3740</td>\n",
              "      <td>88.6011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30639</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Paba</td>\n",
              "      <td>24.4417</td>\n",
              "      <td>88.6278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30640</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Bhuapur</td>\n",
              "      <td>24.4604</td>\n",
              "      <td>89.8727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30641</th>\n",
              "      <td>2020</td>\n",
              "      <td>Protests</td>\n",
              "      <td>Dhaka</td>\n",
              "      <td>23.7406</td>\n",
              "      <td>90.3943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30642</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Bogra</td>\n",
              "      <td>24.8510</td>\n",
              "      <td>89.3711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30643</th>\n",
              "      <td>2020</td>\n",
              "      <td>Riots</td>\n",
              "      <td>Dhaka</td>\n",
              "      <td>23.7333</td>\n",
              "      <td>90.4000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30644 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       YEAR EVENT_TYPE   LOCATION  LATITUDE  LONGITUDE\n",
              "0      2001    Battles   Lohagara   23.1998    89.6644\n",
              "1      2001      Riots  Hathazari   22.5052    91.8134\n",
              "2      2001      Riots  Rangamati   22.6432    92.1919\n",
              "3      2001      Riots     Rajoir   23.1878    90.0322\n",
              "4      2001      Riots   Rajshahi   24.3740    88.6011\n",
              "...     ...        ...        ...       ...        ...\n",
              "30639  2020      Riots       Paba   24.4417    88.6278\n",
              "30640  2020      Riots    Bhuapur   24.4604    89.8727\n",
              "30641  2020   Protests     Dhaka    23.7406    90.3943\n",
              "30642  2020      Riots      Bogra   24.8510    89.3711\n",
              "30643  2020      Riots     Dhaka    23.7333    90.4000\n",
              "\n",
              "[30644 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3402
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7_hwtDM3rLU"
      },
      "source": [
        "# x = data.copy()\n",
        "# x = x.drop('YEAR',axis=1)\n",
        "# x"
      ],
      "execution_count": 3403,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_crY-gUTL6LN",
        "outputId": "4e57eace-b5af-4bd6-eb6d-63a57bcb1333"
      },
      "source": [
        "x = data[['YEAR','EVENT_TYPE','LOCATION']]\n",
        "# x = data[['YEAR','LOCATION','LATITUDE','LONGITUDE']]\n",
        "# x = x.drop('EVENT_TYPE', axis = 1)\n",
        "y = data[['LATITUDE','LONGITUDE']]\n",
        "# y = data[['EVENT_TYPE']]\n",
        "\n",
        "# x = data.loc[:, data.columns != ('EVENT_TYPE',)]\n",
        "# y = data.loc[:, data.columns == 'EVENT_TYPE']\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 3404,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       YEAR EVENT_TYPE   LOCATION\n",
            "0      2001    Battles   Lohagara\n",
            "1      2001      Riots  Hathazari\n",
            "2      2001      Riots  Rangamati\n",
            "3      2001      Riots     Rajoir\n",
            "4      2001      Riots   Rajshahi\n",
            "...     ...        ...        ...\n",
            "30639  2020      Riots       Paba\n",
            "30640  2020      Riots    Bhuapur\n",
            "30641  2020   Protests     Dhaka \n",
            "30642  2020      Riots      Bogra\n",
            "30643  2020      Riots     Dhaka \n",
            "\n",
            "[30644 rows x 3 columns]\n",
            "       LATITUDE  LONGITUDE\n",
            "0       23.1998    89.6644\n",
            "1       22.5052    91.8134\n",
            "2       22.6432    92.1919\n",
            "3       23.1878    90.0322\n",
            "4       24.3740    88.6011\n",
            "...         ...        ...\n",
            "30639   24.4417    88.6278\n",
            "30640   24.4604    89.8727\n",
            "30641   23.7406    90.3943\n",
            "30642   24.8510    89.3711\n",
            "30643   23.7333    90.4000\n",
            "\n",
            "[30644 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxjVoeVW-hQW"
      },
      "source": [
        "# USE THIS OTHERWISE LE.FIT CAUSES PROBLEMS\n",
        "unique_events = data['EVENT_TYPE'].unique()\n",
        "unique_locations = data['LOCATION'].unique()"
      ],
      "execution_count": 3405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca56odIXlnqK",
        "outputId": "076afd2b-98b8-44e1-dd2b-cced602a3e9a"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from pandas.api.types import is_string_dtype\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "\n",
        "# x_numpy = x.to_numpy()\n",
        "# y_numpy = y.to_numpy()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
        "# stratify=y_numpy\n",
        "\n",
        "\n",
        "# MULTI OUTPUT REGRESSION MODELS: LinearRegression, KNeighborsRegressor, DecisionTreeRegressor,RandomForestRegressor\n",
        "\n",
        "models = [\n",
        "          ('MLP', MLPRegressor(random_state=0, max_iter = 2000)),\n",
        "          # ('Linear Regression', LinearRegression()),\n",
        "          # ('Random Forest', RandomForestClassifier()),\n",
        "          # ('Decision Tree', DecisionTreeClassifier()),\n",
        "          # ('SVM', SVC()),\n",
        "          # ('KNN', KNeighborsClassifier()),\n",
        "          # ('Naive Bayes',GaussianNB()),\n",
        "          # ('Logistics Regression', LogisticRegression()),\n",
        "          # ('Stochastic Gradient Descent', SGDClassifier()),\n",
        "          # ('LightGBM', LGBMClassifier()),\n",
        "          # ('AdaBoost', AdaBoostClassifier()),\n",
        "          # ('XGBoost', XGBClassifier())\n",
        "          ]\n",
        "\n",
        "# single  = [2021, 15, 24, 89]\n",
        "\n",
        "# ms = MinMaxScaler()\n",
        "ms = StandardScaler()\n",
        "le = LabelEncoder()\n",
        "\n",
        "# le.fit(train_y)\n",
        "# train_y = le.transform(train_y)\n",
        "# test_y = le.transform(test_y)\n",
        "\n",
        "ms_train_x = pd.DataFrame()\n",
        "ms_test_x = pd.DataFrame()\n",
        "\n",
        "# LABEL ENCODER\n",
        "for col in x_train.columns:\n",
        "  if (is_string_dtype(x_train[col].dtype)):\n",
        "    uniques = data[col].unique() # Do this otherwise test doesnt get enough labels to labelencode\n",
        "    le.fit(uniques)\n",
        "    ms_train_x[col] = le.transform(x_train[[col]]).flatten() # flatten otherwise 1d doesnt work as numpy array\n",
        "    ms_test_x[col] = le.transform(x_test[[col]]).flatten()\n",
        "\n",
        "  else:\n",
        "    ms_train_x[col] =x_train[col] # flatten otherwise 1d doesnt work as numpy array\n",
        "    ms_test_x[col] = x_test[col]\n",
        "\n",
        "# print(ms_train_x)\n",
        "# print(ms_test_x)\n",
        "# print(y_train)\n",
        "\n",
        "\n",
        "ss_train_x = pd.DataFrame()\n",
        "ss_test_x = pd.DataFrame()\n",
        "\n",
        "# SCALER \n",
        "for col in ms_train_x.columns:\n",
        "  # MIN MAX SCALE X Attributes\n",
        "  ms.fit(ms_train_x[[col]])\n",
        "  ss_train_x[col] = ms.transform(ms_train_x[[col]]).flatten() # flatten otherwise 1d doesnt work as numpy array\n",
        "  ss_test_x[col] = ms.transform(ms_test_x[[col]]).flatten()\n",
        "\n",
        "for col in y_train.columns:\n",
        "  # MIN MAX SCALE X Attributes\n",
        "  ms.fit(y_train[[col]])\n",
        "  y_train[col] = ms.transform(y_train[[col]]).flatten() # flatten otherwise 1d doesnt work as numpy array\n",
        "  y_test[col] = ms.transform(y_test[[col]]).flatten()\n",
        "\n",
        "\n",
        "# print(ss_train_x)\n",
        "# print(ss_test_x)\n",
        "# print(y_train)\n",
        "\n",
        "\n",
        "\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ useless code # all to use\n",
        "# # PREPROCESS LOCATION\n",
        "# le = LabelEncoder()\n",
        "\n",
        "# le.fit(unique_events)\n",
        "# x_train[:,1] = le.transform(x_train[:,1]) # Events\n",
        "# x_test[:,1] = le.transform(x_test[:,1])\n",
        "# print(le.classes_)\n",
        "\n",
        "\n",
        "# le.fit(unique_locations)\n",
        "# x_train[:,2] = le.transform(x_train[:,2]) # Locations\n",
        "# x_test[:,2] = le.transform(x_test[:,2])\n",
        "# print(le.classes_)\n",
        "# # x_train.loc[:, ('LOCATION')] = le.transform(x_train.loc[:, ('LOCATION')]) # IMPORTANT: USE .loc instead of direct ['LOCATION'] otherwise chain indexing error\n",
        "# # x_test.loc[:, ('LOCATION')] = le.transform(x_test.loc[:, ('LOCATION')])\n",
        "# # print(x['LOCATION'])\n",
        "# # single[1] = le.transform(single[1])\n",
        "\n",
        "\n",
        "# # df = pd.DataFrame(single)\n",
        "# # df.loc[1] = le.transform(df.loc[1].values)\n",
        "# # single = df.values.tolist()\n",
        "\n",
        "\n",
        "# # PREPROCESS Y LABELS\n",
        "# # le2 = LabelEncoder()\n",
        "# # le2.fit(unique_events)\n",
        "# # y_train = le2.transform(y_train)\n",
        "# # y_test = le2.transform(y_test)\n",
        "# # print(le2.classes_)\n",
        "\n",
        "# # STANDARD SCALE X Attributes\n",
        "# ss = StandardScaler()\n",
        "# ss.fit(x_train)\n",
        "# x_train = ss.transform(x_train)\n",
        "# x_test = ss.transform(x_test)\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ useless code\n",
        "\n",
        "\n",
        "\n",
        "# df = ss.transform(df)\n",
        "# single = df.values.tolist()\n",
        "\n",
        "for name, model in models:\n",
        "  clf = model.fit(ss_train_x, y_train)\n",
        "  predictions = clf.predict(ss_test_x)\n",
        "\n",
        "  # single_pred = clf.predict([single])\n",
        "\n",
        "  from sklearn.metrics import classification_report\n",
        "  from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "  print(name)\n",
        "  # print(single_pred)\n",
        "  print(predictions)\n",
        "  print('R2 Score', r2_score(y_test, predictions))\n",
        "  print('MSE', mean_squared_error(y_test, predictions, squared= False))\n",
        "  # print('Precision | Recall | F1 Score | Support')\n",
        "  # print(precision_recall_fscore_support(y_test, predictions, average='weighted', labels=np.unique(predictions))) # why did i use np.unique()??\n",
        "  # print(classification_report(y_test,predictions, target_names=(le.classes_).tolist()))\n",
        "  # print(classification_report(y_test,predictions, target_names=(le.classes_).tolist()))\n"
      ],
      "execution_count": 3406,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "MLP\n",
            "[[-0.60194993  0.75715496]\n",
            " [ 0.04194571  0.16832827]\n",
            " [-0.46103981  0.39826847]\n",
            " ...\n",
            " [-0.26810283  0.50780998]\n",
            " [-0.07690296 -0.22127873]\n",
            " [ 0.51225283 -0.10653945]]\n",
            "R2 Score 0.11020488463041139\n",
            "MSE 0.9371949517173791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr5L05wkZ7ip"
      },
      "source": [
        "Problem:\n",
        " * Only 5 labels shown but should be 6 events\n",
        "\n",
        "Ideas:\n",
        "* try adding more col in train y\n",
        "* do the 3 col preds separtely or coord at once then append result\n",
        "* do the lat long first. Add to dataset then do the event type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdXNrp24bONZ"
      },
      "source": [
        "# x = data.loc[:, data.columns != 'EVENT_TYPE']\n",
        "# y = data['EVENT_TYPE']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fou5Oiq0Nvey"
      },
      "source": [
        "# data['EVENT_TYPE'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QskWuIlJM20w"
      },
      "source": [
        "# # x, y = make_multilabel_classification(n_samples = len(data['EVENT_TYPE'].unique()), random_state=0,return_indicator=False)\n",
        "# # this will generate a random multi-label dataset\n",
        "# x, y = make_multilabel_classification(sparse = True, n_labels = 6, return_indicator = 'sparse', allow_unlabeled = False)\n",
        "# # print(x)\n",
        "# # print(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awc95A1YPT_J"
      },
      "source": [
        "# # using binary relevance\n",
        "# from skmultilearn.problem_transform import BinaryRelevance\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# # initialize binary relevance multi-label classifier\n",
        "# # with a gaussian naive bayes base classifier\n",
        "# classifier = BinaryRelevance(GaussianNB())\n",
        "\n",
        "# # train\n",
        "# classifier.fit(x_train, y_train)\n",
        "\n",
        "# # predict\n",
        "# predictions = classifier.predict(x_test)\n",
        "# print(predictions.toarray())\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "# print(classification_report(y_test.toarray(),predictions))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}