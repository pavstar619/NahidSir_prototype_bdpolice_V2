{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NahidSir_prototype_bdpolice_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJ6zinwHc7kvX0AZwuhA++",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavstar619/NahidSir_prototype_bdpolice_V2/blob/main/NahidSir_prototype_bdpolice_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO-1ktbLmRPy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import linear_model \n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fro2ybsI6gqH"
      },
      "source": [
        "**With 2019** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go6OM-_DmbA0"
      },
      "source": [
        "# # dmp = pd.read_csv('dmp.csv') # for dhaka only\n",
        "# dmp = pd.read_excel('bdpolice_totals.xlsx')  \n",
        "# print(dmp)\n",
        "\n",
        "# # dmp = dmp.drop(9) # renove 2019 as data not accurate\n",
        "# dmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF9v2XbLnn6s"
      },
      "source": [
        "# # dmp_x = dmp.loc[:, dmp.columns != 'Robbery']\n",
        "# # dmp_y = dmp['Robbery']\n",
        "# for col in dmp.columns[1:].unique():\n",
        "#   print('~~~~~~~~~~~~~~~~~~~~~', col, '~~~~~~~~~~~~~~~~~~~~~')\n",
        "#   dmp_x = dmp['Year'].values\n",
        "#   dmp_y = dmp[col].values\n",
        "\n",
        "#   train_x, test_x, train_y, test_y = train_test_split(dmp_x, dmp_y, test_size = 0.25, random_state=1)\n",
        "\n",
        "#   # Standard scaling\n",
        "#   ss = StandardScaler()\n",
        "#   ss.fit(train_x.reshape(-1, 1))\n",
        "#   ss_train_x = ss.transform(train_x.reshape(-1, 1))\n",
        "#   ss_test_x = ss.transform(test_x.reshape(-1, 1))\n",
        "\n",
        "#   print(dmp_x)\n",
        "#   print(dmp_y)\n",
        "\n",
        "\n",
        "\n",
        "#   # pred for single data\n",
        "#   Xnew = [[2020]]\n",
        "#   print(len(Xnew[0]))\n",
        "#   # print(len(dmp_x.columns))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   regressor = DecisionTreeRegressor(random_state=0)\n",
        "#   forest = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "#   svr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
        "#   mlp = MLPClassifier(random_state=0, max_iter = 2000)\n",
        "#   lasso = linear_model.Lasso(alpha=0.1)\n",
        "#   bayesian = linear_model.BayesianRidge()\n",
        "#   ridge = linear_model.Ridge()\n",
        "#   lr = LinearRegression()\n",
        "\n",
        "#   regressor.fit(train_x.reshape(-1, 1),train_y)  \n",
        "#   forest.fit(train_x.reshape(-1, 1),train_y) \n",
        "#   svr.fit(train_x.reshape(-1, 1),train_y)\n",
        "#   mlp.fit(ss_train_x.reshape(-1,1),train_y) \n",
        "#   lasso.fit(train_x.reshape(-1,1),train_y) \n",
        "#   bayesian.fit(train_x.reshape(-1,1),train_y) \n",
        "#   ridge.fit(train_x.reshape(-1,1),train_y) \n",
        "#   lr.fit(train_x.reshape(-1,1),train_y)\n",
        "\n",
        "\n",
        "#   pred_y = regressor.predict(test_x.reshape(-1, 1))\n",
        "#   pred_y1 = forest.predict(test_x.reshape(-1, 1)) \n",
        "#   pred_y2 = svr.predict(test_x.reshape(-1, 1)) \n",
        "#   pred_y3 = mlp.predict(ss_test_x.reshape(-1, 1))\n",
        "#   pred_y4 = lasso.predict(test_x.reshape(-1,1))\n",
        "#   pred_y5 = bayesian.predict(test_x.reshape(-1,1))\n",
        "#   pred_y6 = ridge.predict(test_x.reshape(-1,1))\n",
        "#   pred_y7 = ridge.predict(test_x.reshape(-1,1))\n",
        "\n",
        "#   error_y = (pred_y - test_y) / test_y #lower the better\n",
        "#   error_y1 = (pred_y1 - test_y) / test_y\n",
        "#   error_y2 = (pred_y2 - test_y) / test_y\n",
        "#   error_y3 = (pred_y3 - test_y) / test_y\n",
        "#   error_y4 = (pred_y4 - test_y) / test_y\n",
        "#   error_y5 = (pred_y5 - test_y) / test_y\n",
        "#   error_y6 = (pred_y6 - test_y) / test_y\n",
        "#   error_y7 = (pred_y7 - test_y) / test_y\n",
        "\n",
        "\n",
        "#   Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "#   # m2_y = mean_squared_error(test_y,pred_y, squared=False)\n",
        "\n",
        "#   # print(test_y)\n",
        "#   print('Test DT: ',test_y,pred_y,error_y) # Decision Tree\n",
        "#   print('Test RF: ',test_y,pred_y1,error_y1) # Random Forest\n",
        "#   print('Test SVR: ',test_y,pred_y2,error_y2) # SVM\n",
        "#   print('Test MLP: ',test_y,pred_y3,error_y3) # mlp\n",
        "#   print('Test Lasso: ',test_y,pred_y4,error_y4) # lasso\n",
        "#   print('Test Bayesian: ',test_y,pred_y5,error_y5) # bayesian\n",
        "#   print('Test Ridge: ',test_y,pred_y6,error_y6) # ridge\n",
        "#   print('Test Linear Regression: ',test_y,pred_y7,error_y7) # lr\n",
        "\n",
        "#   print('2019 BEST MLP: ',Xnew,Xpred_y) # Decision Tree\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7dILG1AdJk7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjCzsHmk6lOp"
      },
      "source": [
        "**Without 2019**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "_nn-Sam9CwsF",
        "outputId": "6431bb7b-2da4-462b-c674-6c1f923925fb"
      },
      "source": [
        "# dmp = pd.read_csv('dmp.csv')\n",
        "dmp = pd.read_excel('bdpolice_totals.xlsx')  \n",
        "print(dmp)\n",
        "\n",
        "dmp = dmp.drop(9) # renove 2019 as data not accurate\n",
        "dmp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Year  Dacoity  Robbery  Murder  ...  Explosive  Narcotics  Smuggling  Other Cases\n",
            "0  2010      656     1059    3988  ...        253      29344       6363        87139\n",
            "1  2011      650     1069    3966  ...        207      31696       5714        88355\n",
            "2  2012      593      964    4114  ...        289      37264       6578        96112\n",
            "3  2013      613     1021    4393  ...       1007      35832       6437        93930\n",
            "4  2014      651     1155    4514  ...        520      42501       6788        90400\n",
            "5  2015      492      933    4037  ...        725      47666       6179        84117\n",
            "6  2016      408      722    3591  ...        487      62208       4680        77747\n",
            "7  2017      336      657    3549  ...        362      98984       5599        74645\n",
            "8  2018      262      562    3830  ...       1310     112549       4501        69736\n",
            "9  2019       32       68     351  ...         30       9069        361         5428\n",
            "\n",
            "[10 rows x 16 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Dacoity</th>\n",
              "      <th>Robbery</th>\n",
              "      <th>Murder</th>\n",
              "      <th>Speedy Trial</th>\n",
              "      <th>Riot</th>\n",
              "      <th>Woman &amp; Child Repression</th>\n",
              "      <th>Kidnapping</th>\n",
              "      <th>Police Assault</th>\n",
              "      <th>Burglary</th>\n",
              "      <th>Theft</th>\n",
              "      <th>Arms Act</th>\n",
              "      <th>Explosive</th>\n",
              "      <th>Narcotics</th>\n",
              "      <th>Smuggling</th>\n",
              "      <th>Other Cases</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010</td>\n",
              "      <td>656</td>\n",
              "      <td>1059</td>\n",
              "      <td>3988</td>\n",
              "      <td>1666</td>\n",
              "      <td>130</td>\n",
              "      <td>17752</td>\n",
              "      <td>870</td>\n",
              "      <td>473</td>\n",
              "      <td>3101</td>\n",
              "      <td>8529</td>\n",
              "      <td>1575</td>\n",
              "      <td>253</td>\n",
              "      <td>29344</td>\n",
              "      <td>6363</td>\n",
              "      <td>87139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>650</td>\n",
              "      <td>1069</td>\n",
              "      <td>3966</td>\n",
              "      <td>1863</td>\n",
              "      <td>109</td>\n",
              "      <td>21389</td>\n",
              "      <td>792</td>\n",
              "      <td>581</td>\n",
              "      <td>3134</td>\n",
              "      <td>8873</td>\n",
              "      <td>1269</td>\n",
              "      <td>207</td>\n",
              "      <td>31696</td>\n",
              "      <td>5714</td>\n",
              "      <td>88355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012</td>\n",
              "      <td>593</td>\n",
              "      <td>964</td>\n",
              "      <td>4114</td>\n",
              "      <td>1907</td>\n",
              "      <td>94</td>\n",
              "      <td>20947</td>\n",
              "      <td>850</td>\n",
              "      <td>659</td>\n",
              "      <td>2927</td>\n",
              "      <td>8598</td>\n",
              "      <td>1511</td>\n",
              "      <td>289</td>\n",
              "      <td>37264</td>\n",
              "      <td>6578</td>\n",
              "      <td>96112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>613</td>\n",
              "      <td>1021</td>\n",
              "      <td>4393</td>\n",
              "      <td>1896</td>\n",
              "      <td>172</td>\n",
              "      <td>19601</td>\n",
              "      <td>879</td>\n",
              "      <td>1257</td>\n",
              "      <td>2762</td>\n",
              "      <td>7882</td>\n",
              "      <td>1517</td>\n",
              "      <td>1007</td>\n",
              "      <td>35832</td>\n",
              "      <td>6437</td>\n",
              "      <td>93930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>651</td>\n",
              "      <td>1155</td>\n",
              "      <td>4514</td>\n",
              "      <td>1716</td>\n",
              "      <td>79</td>\n",
              "      <td>21291</td>\n",
              "      <td>920</td>\n",
              "      <td>702</td>\n",
              "      <td>2809</td>\n",
              "      <td>7660</td>\n",
              "      <td>2023</td>\n",
              "      <td>520</td>\n",
              "      <td>42501</td>\n",
              "      <td>6788</td>\n",
              "      <td>90400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015</td>\n",
              "      <td>492</td>\n",
              "      <td>933</td>\n",
              "      <td>4037</td>\n",
              "      <td>1549</td>\n",
              "      <td>93</td>\n",
              "      <td>21210</td>\n",
              "      <td>805</td>\n",
              "      <td>634</td>\n",
              "      <td>2495</td>\n",
              "      <td>6821</td>\n",
              "      <td>2079</td>\n",
              "      <td>725</td>\n",
              "      <td>47666</td>\n",
              "      <td>6179</td>\n",
              "      <td>84117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016</td>\n",
              "      <td>408</td>\n",
              "      <td>722</td>\n",
              "      <td>3591</td>\n",
              "      <td>1052</td>\n",
              "      <td>53</td>\n",
              "      <td>18446</td>\n",
              "      <td>639</td>\n",
              "      <td>521</td>\n",
              "      <td>2213</td>\n",
              "      <td>6110</td>\n",
              "      <td>2291</td>\n",
              "      <td>487</td>\n",
              "      <td>62208</td>\n",
              "      <td>4680</td>\n",
              "      <td>77747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2017</td>\n",
              "      <td>336</td>\n",
              "      <td>657</td>\n",
              "      <td>3549</td>\n",
              "      <td>1045</td>\n",
              "      <td>23</td>\n",
              "      <td>17073</td>\n",
              "      <td>509</td>\n",
              "      <td>543</td>\n",
              "      <td>2163</td>\n",
              "      <td>5833</td>\n",
              "      <td>2208</td>\n",
              "      <td>362</td>\n",
              "      <td>98984</td>\n",
              "      <td>5599</td>\n",
              "      <td>74645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018</td>\n",
              "      <td>262</td>\n",
              "      <td>562</td>\n",
              "      <td>3830</td>\n",
              "      <td>922</td>\n",
              "      <td>26</td>\n",
              "      <td>16253</td>\n",
              "      <td>444</td>\n",
              "      <td>811</td>\n",
              "      <td>2137</td>\n",
              "      <td>5561</td>\n",
              "      <td>2515</td>\n",
              "      <td>1310</td>\n",
              "      <td>112549</td>\n",
              "      <td>4501</td>\n",
              "      <td>69736</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year  Dacoity  Robbery  Murder  ...  Explosive  Narcotics  Smuggling  Other Cases\n",
              "0  2010      656     1059    3988  ...        253      29344       6363        87139\n",
              "1  2011      650     1069    3966  ...        207      31696       5714        88355\n",
              "2  2012      593      964    4114  ...        289      37264       6578        96112\n",
              "3  2013      613     1021    4393  ...       1007      35832       6437        93930\n",
              "4  2014      651     1155    4514  ...        520      42501       6788        90400\n",
              "5  2015      492      933    4037  ...        725      47666       6179        84117\n",
              "6  2016      408      722    3591  ...        487      62208       4680        77747\n",
              "7  2017      336      657    3549  ...        362      98984       5599        74645\n",
              "8  2018      262      562    3830  ...       1310     112549       4501        69736\n",
              "\n",
              "[9 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vZ_uSyOC2lr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e50e40f-625b-4a78-c765-72be3db5a65a"
      },
      "source": [
        "dmp_2018 = dmp.iloc[8:]\n",
        "print(dmp_2018)\n",
        "\n",
        "dmp = dmp.drop(8)\n",
        "print(dmp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Year  Dacoity  Robbery  Murder  ...  Explosive  Narcotics  Smuggling  Other Cases\n",
            "8  2018      262      562    3830  ...       1310     112549       4501        69736\n",
            "\n",
            "[1 rows x 16 columns]\n",
            "   Year  Dacoity  Robbery  Murder  ...  Explosive  Narcotics  Smuggling  Other Cases\n",
            "0  2010      656     1059    3988  ...        253      29344       6363        87139\n",
            "1  2011      650     1069    3966  ...        207      31696       5714        88355\n",
            "2  2012      593      964    4114  ...        289      37264       6578        96112\n",
            "3  2013      613     1021    4393  ...       1007      35832       6437        93930\n",
            "4  2014      651     1155    4514  ...        520      42501       6788        90400\n",
            "5  2015      492      933    4037  ...        725      47666       6179        84117\n",
            "6  2016      408      722    3591  ...        487      62208       4680        77747\n",
            "7  2017      336      657    3549  ...        362      98984       5599        74645\n",
            "\n",
            "[8 rows x 16 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pc-bosPC2xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6ba257-a44d-4ceb-96d7-9d0b06fd32e3"
      },
      "source": [
        "# dmp_x = dmp.loc[:, dmp.columns != 'Robbery']\n",
        "# dmp_y = dmp['Robbery']\n",
        "for col in dmp.columns[1:].unique():\n",
        "  print('~~~~~~~~~~~~~~~~~~~~~', col, '~~~~~~~~~~~~~~~~~~~~~')\n",
        "  dmp_x = dmp['Year'].values\n",
        "  dmp_y = dmp[col].values\n",
        "\n",
        "  train_x, test_x, train_y, test_y = train_test_split(dmp_x, dmp_y, test_size = 0.25, random_state=0)\n",
        "\n",
        "  # Standard scaling\n",
        "  ss = StandardScaler()\n",
        "  ss.fit(train_x.reshape(-1, 1))\n",
        "  ss_train_x = ss.transform(train_x.reshape(-1, 1))\n",
        "  ss_test_x = ss.transform(test_x.reshape(-1, 1))\n",
        "\n",
        "  print(train_x)\n",
        "  print(test_x)\n",
        "\n",
        "\n",
        "\n",
        "  # pred for single data\n",
        "  Xnew = [[2018]]\n",
        "  print(len(Xnew[0]))\n",
        "  # print(len(dmp_x.columns))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  regressor = DecisionTreeRegressor(random_state=0)\n",
        "  forest = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "  svr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
        "  mlp = MLPClassifier(random_state=0, max_iter = 2000)\n",
        "  lasso = linear_model.Lasso(alpha=0.1)\n",
        "  bayesian = linear_model.BayesianRidge()\n",
        "  ridge = linear_model.Ridge()\n",
        "  lr = LinearRegression()\n",
        "\n",
        "  regressor.fit(train_x.reshape(-1, 1),train_y)  \n",
        "  forest.fit(train_x.reshape(-1, 1),train_y) \n",
        "  svr.fit(train_x.reshape(-1, 1),train_y)\n",
        "  mlp.fit(ss_train_x.reshape(-1,1),train_y) \n",
        "  lasso.fit(train_x.reshape(-1,1),train_y) \n",
        "  bayesian.fit(train_x.reshape(-1,1),train_y) \n",
        "  ridge.fit(train_x.reshape(-1,1),train_y) \n",
        "  lr.fit(train_x.reshape(-1,1),train_y)\n",
        "\n",
        "\n",
        "  pred_y = regressor.predict(test_x.reshape(-1, 1))\n",
        "  pred_y1 = forest.predict(test_x.reshape(-1, 1)) \n",
        "  pred_y2 = svr.predict(test_x.reshape(-1, 1)) \n",
        "  pred_y3 = mlp.predict(ss_test_x.reshape(-1, 1))\n",
        "  pred_y4 = lasso.predict(test_x.reshape(-1,1))\n",
        "  pred_y5 = bayesian.predict(test_x.reshape(-1,1))\n",
        "  pred_y6 = ridge.predict(test_x.reshape(-1,1))\n",
        "  pred_y7 = lr.predict(test_x.reshape(-1,1))\n",
        "\n",
        "  error_y = (pred_y - test_y) / test_y #lower the better\n",
        "  error_y1 = (pred_y1 - test_y) / test_y\n",
        "  error_y2 = (pred_y2 - test_y) / test_y\n",
        "  error_y3 = (pred_y3 - test_y) / test_y\n",
        "  error_y4 = (pred_y4 - test_y) / test_y\n",
        "  error_y5 = (pred_y5 - test_y) / test_y\n",
        "  error_y6 = (pred_y6 - test_y) / test_y\n",
        "  error_y7 = (pred_y7 - test_y) / test_y\n",
        "\n",
        "\n",
        "  Xpred_y = mlp.predict(Xnew) #one data only\n",
        "\n",
        "\n",
        "  # m2_y = mean_squared_error(test_y,pred_y, squared=False)\n",
        "\n",
        "  # print(test_y)\n",
        "  print('Test DT: ',test_y,pred_y,error_y) # Decision Tree\n",
        "  print('Test RF: ',test_y,pred_y1,error_y1) # Random Forest\n",
        "  print('Test SVR: ',test_y,pred_y2,error_y2) # SVM\n",
        "  print('Test MLP: ',test_y,pred_y3,error_y3) # mlp\n",
        "  print('Test Lasso: ',test_y,pred_y4,error_y4) # lasso\n",
        "  print('Test Bayesian: ',test_y,pred_y5,error_y5) # bayesian\n",
        "  print('Test Ridge: ',test_y,pred_y6,error_y6) # ridge\n",
        "  print('Test Linear Regression: ',test_y,pred_y7,error_y7) # lr\n",
        "\n",
        "  print('2018 BEST MLP: ',Xnew,Xpred_y) # Decision Tree\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~ Dacoity ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2012 2013 2010 2011 2014 2017]\n",
            "[2016 2015]\n",
            "1\n",
            "Test DT:  [408 492] [336. 651.] [-0.17647059  0.32317073]\n",
            "Test RF:  [408 492] [430.79       609.16916667] [0.05585784 0.23814871]\n",
            "Test SVR:  [408 492] [630.77575419 631.13262296] [0.54601901 0.28278988]\n",
            "Test MLP:  [408 492] [336 651] [-0.17647059  0.32317073]\n",
            "Test Lasso:  [408 492] [451.47783784 493.06378378] [0.10656333 0.00216216]\n",
            "Test Bayesian:  [408 492] [462.8098696 500.8172792] [0.13433792 0.0179213 ]\n",
            "Test Ridge:  [408 492] [455.55497382 495.85340314] [0.11655631 0.00783212]\n",
            "Test Linear Regression:  [408 492] [451.41621622 493.02162162] [0.10641229 0.00207647]\n",
            "2018 BEST MLP:  [[2018]] [336]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Robbery ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2017 2012 2011 2010 2015 2014]\n",
            "[2016 2013]\n",
            "1\n",
            "Test DT:  [ 722 1021] [933. 964.] [ 0.29224377 -0.05582762]\n",
            "Test RF:  [ 722 1021] [ 909.63833333 1037.42333333] [0.25988689 0.01608554]\n",
            "Test SVR:  [ 722 1021] [1009.26561397 1010.64375839] [ 0.39787481 -0.01014323]\n",
            "Test MLP:  [ 722 1021] [657 964] [-0.0900277  -0.05582762]\n",
            "Test Lasso:  [ 722 1021] [842.50813397 980.49952153] [ 0.16690877 -0.03966746]\n",
            "Test Bayesian:  [ 722 1021] [869.86575754 978.89024956] [ 0.20480022 -0.04124363]\n",
            "Test Ridge:  [ 722 1021] [846.09767442 980.28837209] [ 0.17188044 -0.03987427]\n",
            "Test Linear Regression:  [ 722 1021] [842.45933014 980.50239234] [ 0.16684118 -0.03966465]\n",
            "2018 BEST MLP:  [[2018]] [657]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Murder ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2010 2017 2013 2015 2011 2016]\n",
            "[2014 2012]\n",
            "1\n",
            "Test DT:  [4514 4114] [4393. 3966.] [-0.02680549 -0.03597472]\n",
            "Test RF:  [4514 4114] [4207.25666667 4060.525     ] [-0.06795377 -0.0129983 ]\n",
            "Test SVR:  [4514 4114] [3977.31667336 3977.25000038] [-0.11889307 -0.03324016]\n",
            "Test MLP:  [4514 4114] [4037 3966] [-0.10567125 -0.03597472]\n",
            "Test Lasso:  [4514 4114] [3899.18305085 4028.08474576] [-0.13620225 -0.02088363]\n",
            "Test Bayesian:  [4514 4114] [3907.76419486 3985.17902572] [-0.13430124 -0.03131283]\n",
            "Test Ridge:  [4514 4114] [3899.7107438  4025.44628099] [-0.13608535 -0.02152497]\n",
            "Test Linear Regression:  [4514 4114] [3899.1779661  4028.11016949] [-0.13620337 -0.02087745]\n",
            "2018 BEST MLP:  [[2018]] [3549]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Speedy Trial ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2014 2013 2011 2017 2016 2012]\n",
            "[2010 2015]\n",
            "1\n",
            "Test DT:  [1666 1549] [1863. 1716.] [0.1182473  0.10781149]\n",
            "Test RF:  [1666 1549] [1878.43916667 1528.19166667] [ 0.12751451 -0.0134334 ]\n",
            "Test SVR:  [1666 1549] [1789.8786699 1787.1271421] [0.07435694 0.15372959]\n",
            "Test MLP:  [1666 1549] [1863 1052] [ 0.1182473  -0.32085216]\n",
            "Test Lasso:  [1666 1549] [2219.77142857 1385.06956522] [ 0.33239582 -0.10582985]\n",
            "Test Bayesian:  [1666 1549] [2198.91865125 1391.41606266] [ 0.31987914 -0.10173269]\n",
            "Test Ridge:  [1666 1549] [2196.86227545 1392.04191617] [ 0.31864482 -0.10132865]\n",
            "Test Linear Regression:  [1666 1549] [2219.85714286 1385.04347826] [ 0.33244726 -0.10584669]\n",
            "2018 BEST MLP:  [[2018]] [1045]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Riot ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2010 2011 2016 2012 2017 2014]\n",
            "[2013 2015]\n",
            "1\n",
            "Test DT:  [172  93] [94. 79.] [-0.45348837 -0.15053763]\n",
            "Test RF:  [172  93] [91.605  72.4275] [-0.46741279 -0.22120968]\n",
            "Test SVR:  [172  93] [86.53793048 84.18401677] [-0.4968725  -0.09479552]\n",
            "Test MLP:  [172  93] [94 53] [-0.45348837 -0.43010753]\n",
            "Test Lasso:  [172  93] [85.85932203 58.70338983] [-0.5008179  -0.36878075]\n",
            "Test Bayesian:  [172  93] [85.83479432 58.8260284 ] [-0.5009605  -0.36746206]\n",
            "Test Ridge:  [172  93] [85.75206612 59.23966942] [-0.50144148 -0.36301431]\n",
            "Test Linear Regression:  [172  93] [85.86440678 58.6779661 ] [-0.50078833 -0.36905413]\n",
            "2018 BEST MLP:  [[2018]] [23]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Woman & Child Repression ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2015 2013 2011 2012 2017 2016]\n",
            "[2014 2010]\n",
            "1\n",
            "Test DT:  [21291 17752] [19601. 21389.] [-0.07937626  0.20487832]\n",
            "Test RF:  [21291 17752] [20294.98666667 21060.825     ] [-0.04678096  0.18639167]\n",
            "Test SVR:  [21291 17752] [20273.20685869 20274.297234  ] [-0.04780391  0.14208524]\n",
            "Test MLP:  [21291 17752] [21210 21389] [-0.00380442  0.20487832]\n",
            "Test Lasso:  [21291 17752] [19777.66666667 22112.00952381] [-0.07107855  0.24560667]\n",
            "Test Bayesian:  [21291 17752] [19777.66666667 21838.0179942 ] [-0.07107855  0.23017226]\n",
            "Test Ridge:  [21291 17752] [19777.66666667 22031.59770115] [-0.07107855  0.24107693]\n",
            "Test Linear Regression:  [21291 17752] [19777.66666667 22112.0952381 ] [-0.07107855  0.24561149]\n",
            "2018 BEST MLP:  [[2018]] [17073]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Kidnapping ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2016 2010 2013 2011 2014 2017]\n",
            "[2015 2012]\n",
            "1\n",
            "Test DT:  [805 850] [920. 792.] [ 0.14285714 -0.06823529]\n",
            "Test RF:  [805 850] [838.34916667 823.4625    ] [ 0.04142754 -0.03122059]\n",
            "Test SVR:  [805 850] [830.41429031 831.43224389] [ 0.03157055 -0.02184442]\n",
            "Test MLP:  [805 850] [639 879] [-0.20621118  0.03411765]\n",
            "Test Lasso:  [805 850] [703.17066667 833.16266667] [-0.12649607 -0.01980863]\n",
            "Test Bayesian:  [805 850] [714.15876179 822.17457154] [-0.11284626 -0.0327358 ]\n",
            "Test Ridge:  [805 850] [704.83549784 831.4978355 ] [-0.12442795 -0.02176725]\n",
            "Test Linear Regression:  [805 850] [703.14666667 833.18666667] [-0.12652588 -0.01978039]\n",
            "2018 BEST MLP:  [[2018]] [509]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Police Assault ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2015 2012 2011 2014 2010 2017]\n",
            "[2016 2013]\n",
            "1\n",
            "Test DT:  [ 521 1257] [634. 659.] [ 0.2168906  -0.47573588]\n",
            "Test RF:  [ 521 1257] [618.2125     658.90083333] [ 0.18658829 -0.47581477]\n",
            "Test SVR:  [ 521 1257] [607.67582911 608.53350963] [ 0.16636436 -0.51588424]\n",
            "Test MLP:  [ 521 1257] [543 702] [ 0.04222649 -0.44152745]\n",
            "Test Lasso:  [ 521 1257] [623.29090909 597.21818182] [ 0.19633572 -0.52488609]\n",
            "Test Bayesian:  [ 521 1257] [598.6702801  598.66645411] [ 0.14907923 -0.52373393]\n",
            "Test Ridge:  [ 521 1257] [622.65116279 597.25581395] [ 0.1951078  -0.52485615]\n",
            "Test Linear Regression:  [ 521 1257] [623.33971292 597.215311  ] [ 0.19642939 -0.52488838]\n",
            "2018 BEST MLP:  [[2018]] [543]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Burglary ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2013 2012 2011 2016 2010 2014]\n",
            "[2017 2015]\n",
            "1\n",
            "Test DT:  [2163 2495] [2213. 2809.] [0.02311604 0.1258517 ]\n",
            "Test RF:  [2163 2495] [2363.03333333 2660.23      ] [0.09247958 0.06622445]\n",
            "Test SVR:  [2163 2495] [2867.45049194 2866.54996679] [0.32568215 0.14891782]\n",
            "Test MLP:  [2163 2495] [2213 2213] [ 0.02311604 -0.11302605]\n",
            "Test Lasso:  [2163 2495] [2192.76857143 2484.26      ] [ 0.01376263 -0.00430461]\n",
            "Test Bayesian:  [2163 2495] [2209.22188245 2493.11947516] [ 0.02136934 -0.00075372]\n",
            "Test Ridge:  [2163 2495] [2218.61643836 2498.17808219] [0.02571264 0.00127378]\n",
            "Test Linear Regression:  [2163 2495] [2192.65714286 2484.2       ] [ 0.01371112 -0.00432866]\n",
            "2018 BEST MLP:  [[2018]] [2213]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Theft ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2013 2012 2014 2011 2017 2016]\n",
            "[2015 2010]\n",
            "1\n",
            "Test DT:  [6821 8529] [7660. 8873.] [0.12300249 0.04033298]\n",
            "Test RF:  [6821 8529] [7248.055 8688.62 ] [0.06260886 0.01871497]\n",
            "Test SVR:  [6821 8529] [7769.03890236 7771.79043017] [ 0.13898826 -0.08878058]\n",
            "Test MLP:  [6821 8529] [7660 8873] [0.12300249 0.04033298]\n",
            "Test Lasso:  [6821 8529] [6862.89565217 9561.91428571] [0.00614216 0.12110614]\n",
            "Test Bayesian:  [6821 8529] [6865.33997064 9553.88295361] [0.00650051 0.12016449]\n",
            "Test Ridge:  [6821 8529] [6885.49700599 9487.65269461] [0.00945565 0.11239919]\n",
            "Test Linear Regression:  [6821 8529] [6862.86956522 9562.        ] [0.00613833 0.12111619]\n",
            "2018 BEST MLP:  [[2018]] [5833]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Arms Act ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2013 2012 2015 2014 2016 2011]\n",
            "[2017 2010]\n",
            "1\n",
            "Test DT:  [2208 1575] [2291. 1269.] [ 0.03759058 -0.19428571]\n",
            "Test RF:  [2208 1575] [2207.7  1349.62] [-1.35869565e-04 -1.43098413e-01]\n",
            "Test SVR:  [2208 1575] [1771.00484181 1768.99515819] [-0.19791447  0.12317153]\n",
            "Test MLP:  [2208 1575] [2291 1269] [ 0.03759058 -0.19428571]\n",
            "Test Lasso:  [2208 1575] [2513.54666667 1049.78666667] [ 0.13838164 -0.33346878]\n",
            "Test Bayesian:  [2208 1575] [2504.72683408 1058.60649926] [ 0.13438715 -0.32786889]\n",
            "Test Ridge:  [2208 1575] [2474.0990991  1089.23423423] [ 0.1205159  -0.30842271]\n",
            "Test Linear Regression:  [2208 1575] [2513.66666667 1049.66666667] [ 0.13843599 -0.33354497]\n",
            "2018 BEST MLP:  [[2018]] [2291]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Explosive ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2014 2016 2011 2013 2010 2012]\n",
            "[2015 2017]\n",
            "1\n",
            "Test DT:  [725 362] [520. 487.] [-0.28275862  0.34530387]\n",
            "Test RF:  [725 362] [550.0825 547.1125] [-0.24126552  0.5113605 ]\n",
            "Test SVR:  [725 362] [389.55992174 388.65939659] [-0.46267597  0.07364474]\n",
            "Test MLP:  [725 362] [487 487] [-0.32827586  0.34530387]\n",
            "Test Lasso:  [725 362] [604.44       727.81714286] [-0.16628966  1.01054459]\n",
            "Test Bayesian:  [725 362] [490.81660874 516.80227337] [-0.32301157  0.42763059]\n",
            "Test Ridge:  [725 362] [598.58219178 716.93835616] [-0.17436939  0.9804927 ]\n",
            "Test Linear Regression:  [725 362] [604.5        727.92857143] [-0.1662069   1.01085241]\n",
            "2018 BEST MLP:  [[2018]] [487]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Narcotics ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2010 2017 2012 2011 2016 2014]\n",
            "[2013 2015]\n",
            "1\n",
            "Test DT:  [35832 47666] [37264. 42501.] [ 0.03996428 -0.10835816]\n",
            "Test RF:  [35832 47666] [37904.08083333 48733.81083333] [0.05782766 0.02240194]\n",
            "Test SVR:  [35832 47666] [39882.46206952 39884.81598323] [ 0.11304036 -0.16324391]\n",
            "Test MLP:  [35832 47666] [37264 62208] [0.03996428 0.30508119]\n",
            "Test Lasso:  [35832 47666] [47487.86101695 64557.69491525] [0.32529195 0.35437618]\n",
            "Test Bayesian:  [35832 47666] [50332.83311207 50332.83443966] [0.40468947 0.05594836]\n",
            "Test Ridge:  [35832 47666] [47558.39256199 64205.03719008] [0.32726034 0.34697766]\n",
            "Test Linear Regression:  [35832 47666] [47487.8559322  64557.72033898] [0.3252918  0.35437671]\n",
            "2018 BEST MLP:  [[2018]] [98984]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Smuggling ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2010 2017 2011 2015 2014 2016]\n",
            "[2013 2012]\n",
            "1\n",
            "Test DT:  [6437 6578] [6788. 5714.] [ 0.05452851 -0.13134691]\n",
            "Test RF:  [6437 6578] [6394.04166667 6045.305     ] [-0.00667366 -0.0809813 ]\n",
            "Test SVR:  [6437 6578] [5946.9909675  5946.58323096] [-0.07612382 -0.09598917]\n",
            "Test MLP:  [6437 6578] [6788 5714] [ 0.05452851 -0.13134691]\n",
            "Test Lasso:  [6437 6578] [5980.94849785 6093.48669528] [-0.07084845 -0.07365663]\n",
            "Test Bayesian:  [6437 6578] [5887.17635706 5887.18798554] [-0.08541613 -0.10501855]\n",
            "Test Ridge:  [6437 6578] [5978.60669456 6088.33472803] [-0.07121226 -0.07443984]\n",
            "Test Linear Regression:  [6437 6578] [5980.96137339 6093.51502146] [-0.07084645 -0.07365232]\n",
            "2018 BEST MLP:  [[2018]] [5599]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Other Cases ~~~~~~~~~~~~~~~~~~~~~\n",
            "[2011 2017 2014 2016 2012 2013]\n",
            "[2010 2015]\n",
            "1\n",
            "Test DT:  [87139 84117] [88355. 90400.] [0.01395472 0.07469358]\n",
            "Test RF:  [87139 84117] [91076.39833333 87375.81833333] [0.04518526 0.03874149]\n",
            "Test SVR:  [87139 84117] [89376.33528716 89376.67052483] [0.02567548 0.06252803]\n",
            "Test MLP:  [87139 84117] [88355 90400] [0.01395472 0.07469358]\n",
            "Test Lasso:  [87139 84117] [98996.91428571 83172.46086956] [ 0.13608045 -0.01122887]\n",
            "Test Bayesian:  [87139 84117] [86864.84546247 86864.82964186] [-0.00314617  0.03266676]\n",
            "Test Ridge:  [87139 84117] [98561.11377245 83305.09580838] [ 0.13107924 -0.00965208]\n",
            "Test Linear Regression:  [87139 84117] [98997.         83172.43478261] [ 0.13608143 -0.01122918]\n",
            "2018 BEST MLP:  [[2018]] [74645]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohmwjbp3Ywge"
      },
      "source": [
        "# # scikit-learn k-fold cross-validation\n",
        "# from numpy import array\n",
        "# from sklearn.model_selection import KFold\n",
        "# # data sample\n",
        "# data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
        "# # prepare cross validation\n",
        "# kfold = KFold(3, True, 1)\n",
        "# # enumerate splits\n",
        "# for train, test in kfold.split(data):\n",
        "# \tprint('train: %s, test: %s' % (data[train], data[test]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLiB5nyQf_Nc",
        "outputId": "dea26b03-f00d-4660-995d-11e2891bac90"
      },
      "source": [
        "kfold = KFold(8, True, 1)\n",
        "\n",
        "for col in dmp.columns[1:].unique():\n",
        "  print('~~~~~~~~~~~~~~~~~~~~~', col, '~~~~~~~~~~~~~~~~~~~~~')\n",
        "  dmp_x = dmp['Year'].values\n",
        "  dmp_y = dmp[col].values\n",
        "\n",
        "  dmp_2018_x = dmp_2018['Year'].values\n",
        "  dmp_2018_y = dmp_2018[col].values\n",
        "\n",
        "  for train_index, test_index in kfold.split(dmp_x):\n",
        "    # all kfolds train test splits\n",
        "    print('train: %s, test: %s' % (dmp_x[train_index], dmp_x[test_index]))\n",
        "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    mlp = MLPClassifier(random_state=0, max_iter = 2000)\n",
        "    mlp.fit(dmp_x[train_index].reshape(-1,1),dmp_y[train_index])\n",
        "    pred_y3 = mlp.predict(dmp_x[test_index].reshape(-1, 1))\n",
        "\n",
        "    # predict 2018\n",
        "    Xpred_y = mlp.predict(dmp_2018_x.reshape(-1, 1))\n",
        "\n",
        "    print('Test MLP: ',dmp_y[test_index],pred_y3) # mlp\n",
        "    # print(precision_recall_fscore_support(dmp_y[test_index], pred_y3, average='macro'))\n",
        "    error_y3 = (pred_y3 - dmp_y[test_index]) / dmp_y[test_index]\n",
        "    print(error_y3)\n",
        "\n",
        "    print('~~~ 2018 BEST MLP ~~~: ',dmp_2018_x[0],dmp_2018_y[0],Xpred_y) # Decision Tree\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~ Dacoity ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [336] [492]\n",
            "[0.46428571]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 262 [492]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [593] [336]\n",
            "[-0.43338954]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 262 [336]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [650] [492]\n",
            "[-0.24307692]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 262 [492]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [408] [651]\n",
            "[0.59558824]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 262 [651]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [656] [492]\n",
            "[-0.25]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 262 [492]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [651] [593]\n",
            "[-0.0890937]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 262 [593]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [613] [408]\n",
            "[-0.33442088]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 262 [408]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [492] [613]\n",
            "[0.24593496]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 262 [613]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Robbery ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [657] [1155]\n",
            "[0.75799087]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 562 [1155]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [964] [1069]\n",
            "[0.10892116]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 562 [1069]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [1069] [1155]\n",
            "[0.08044902]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 562 [1155]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [722] [657]\n",
            "[-0.0900277]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 562 [657]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [1059] [933]\n",
            "[-0.11898017]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 562 [933]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [1155] [933]\n",
            "[-0.19220779]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 562 [933]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [1021] [964]\n",
            "[-0.05582762]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 562 [964]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [933] [1059]\n",
            "[0.13504823]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 562 [1059]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Murder ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [3549] [3988]\n",
            "[0.12369682]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 3830 [3988]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [4114] [3591]\n",
            "[-0.12712688]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 3830 [3591]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [3966] [3549]\n",
            "[-0.10514372]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 3830 [3549]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [3591] [3549]\n",
            "[-0.01169591]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 3830 [3549]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [3988] [4393]\n",
            "[0.10155466]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 3830 [4393]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [4514] [3966]\n",
            "[-0.12140009]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 3830 [3966]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [4393] [3549]\n",
            "[-0.19212383]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 3830 [3549]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [4037] [4393]\n",
            "[0.0881843]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 3830 [4393]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Speedy Trial ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [1045] [1896]\n",
            "[0.81435407]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 922 [1896]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [1907] [1549]\n",
            "[-0.18772942]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 922 [1549]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [1863] [1907]\n",
            "[0.02361782]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 922 [1907]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [1052] [1896]\n",
            "[0.80228137]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 922 [1896]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [1666] [1907]\n",
            "[0.14465786]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 922 [1907]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [1716] [1052]\n",
            "[-0.38694639]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 922 [1052]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [1896] [1863]\n",
            "[-0.01740506]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 922 [1863]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [1549] [1666]\n",
            "[0.0755326]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 922 [1666]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Riot ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [23] [130]\n",
            "[4.65217391]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 26 [130]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [94] [79]\n",
            "[-0.15957447]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 26 [79]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [109] [130]\n",
            "[0.19266055]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 26 [130]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [53] [130]\n",
            "[1.45283019]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 26 [130]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [130] [109]\n",
            "[-0.16153846]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 26 [109]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [79] [53]\n",
            "[-0.32911392]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 26 [53]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [172] [79]\n",
            "[-0.54069767]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 26 [79]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [93] [53]\n",
            "[-0.43010753]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 26 [53]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Woman & Child Repression ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [17073] [19601]\n",
            "[0.14807005]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 16253 [19601]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [20947] [18446]\n",
            "[-0.11939657]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 16253 [18446]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [21389] [21210]\n",
            "[-0.00836879]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 16253 [21210]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [18446] [19601]\n",
            "[0.0626152]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 16253 [19601]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [17752] [17073]\n",
            "[-0.03824921]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 16253 [17073]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [21291] [19601]\n",
            "[-0.07937626]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 16253 [19601]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [19601] [17073]\n",
            "[-0.12897301]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 16253 [17073]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [21210] [19601]\n",
            "[-0.07586044]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 16253 [19601]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Kidnapping ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [509] [879]\n",
            "[0.72691552]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 444 [879]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [850] [509]\n",
            "[-0.40117647]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 444 [509]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [792] [920]\n",
            "[0.16161616]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 444 [920]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [639] [509]\n",
            "[-0.20344288]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 444 [509]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [870] [879]\n",
            "[0.01034483]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 444 [879]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [920] [879]\n",
            "[-0.04456522]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 444 [879]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [879] [850]\n",
            "[-0.03299204]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 444 [850]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [805] [792]\n",
            "[-0.01614907]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 444 [792]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Police Assault ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [543] [521]\n",
            "[-0.04051565]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 811 [521]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [659] [1257]\n",
            "[0.90743551]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 811 [1257]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [581] [702]\n",
            "[0.20826162]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 811 [702]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [521] [543]\n",
            "[0.04222649]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 811 [543]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [473] [1257]\n",
            "[1.65750529]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 811 [1257]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [702] [581]\n",
            "[-0.17236467]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 811 [581]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [1257] [659]\n",
            "[-0.47573588]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 811 [659]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [634] [543]\n",
            "[-0.14353312]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 811 [543]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Burglary ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [2163] [2762]\n",
            "[0.27693019]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2137 [2762]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [2927] [3101]\n",
            "[0.05944653]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2137 [3101]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [3134] [2927]\n",
            "[-0.06604978]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2137 [2927]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [2213] [3101]\n",
            "[0.40126525]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2137 [3101]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [3101] [3134]\n",
            "[0.01064173]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2137 [3134]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [2809] [2163]\n",
            "[-0.22997508]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2137 [2163]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [2762] [2495]\n",
            "[-0.09666908]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2137 [2495]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [2495] [2213]\n",
            "[-0.11302605]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2137 [2213]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Theft ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [5833] [6821]\n",
            "[0.16938111]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 5561 [6821]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [8598] [5833]\n",
            "[-0.32158642]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 5561 [5833]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [8873] [8598]\n",
            "[-0.0309929]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 5561 [8598]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [6110] [8873]\n",
            "[0.45220949]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 5561 [8873]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [8529] [8598]\n",
            "[0.00809005]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 5561 [8598]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [7660] [6821]\n",
            "[-0.10953003]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 5561 [6821]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [7882] [6110]\n",
            "[-0.22481604]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 5561 [6110]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [6821] [5833]\n",
            "[-0.1448468]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 5561 [5833]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Arms Act ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [2208] [1269]\n",
            "[-0.42527174]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2515 [1269]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [1511] [1575]\n",
            "[0.04235606]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2515 [1575]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [1269] [1575]\n",
            "[0.24113475]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2515 [1575]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [2291] [2208]\n",
            "[-0.03622872]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2515 [2208]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [1575] [1511]\n",
            "[-0.04063492]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2515 [1511]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [2023] [1511]\n",
            "[-0.25308947]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2515 [1511]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [1517] [1269]\n",
            "[-0.16348055]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2515 [1269]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [2079] [2291]\n",
            "[0.1019721]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 2515 [2291]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Explosive ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [362] [1007]\n",
            "[1.78176796]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 1310 [1007]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [289] [1007]\n",
            "[2.48442907]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 1310 [1007]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [207] [289]\n",
            "[0.39613527]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 1310 [289]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [487] [1007]\n",
            "[1.06776181]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 1310 [1007]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [253] [520]\n",
            "[1.05533597]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 1310 [520]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [520] [207]\n",
            "[-0.60192308]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 1310 [207]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [1007] [289]\n",
            "[-0.71300894]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 1310 [289]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [725] [362]\n",
            "[-0.50068966]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 1310 [362]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Narcotics ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [98984] [42501]\n",
            "[-0.57062758]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 112549 [42501]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [37264] [29344]\n",
            "[-0.21253757]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 112549 [29344]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [31696] [62208]\n",
            "[0.96264513]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 112549 [62208]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [62208] [35832]\n",
            "[-0.42399691]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 112549 [35832]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [29344] [35832]\n",
            "[0.22110142]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 112549 [35832]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [42501] [35832]\n",
            "[-0.15691395]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 112549 [35832]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [35832] [29344]\n",
            "[-0.1810672]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 112549 [37264]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [47666] [62208]\n",
            "[0.30508119]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 112549 [62208]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Smuggling ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [5599] [6578]\n",
            "[0.17485265]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 4501 [6578]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [6578] [5599]\n",
            "[-0.14882943]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 4501 [5599]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [5714] [6363]\n",
            "[0.11358068]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 4501 [6363]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [4680] [6179]\n",
            "[0.32029915]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 4501 [6179]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [6363] [4680]\n",
            "[-0.26449788]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 4501 [4680]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [6788] [6437]\n",
            "[-0.0517089]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 4501 [6437]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [6437] [5599]\n",
            "[-0.13018487]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 4501 [5599]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [6179] [5714]\n",
            "[-0.0752549]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 4501 [5714]\n",
            "~~~~~~~~~~~~~~~~~~~~~ Other Cases ~~~~~~~~~~~~~~~~~~~~~\n",
            "train: [2010 2011 2012 2013 2014 2015 2016], test: [2017]\n",
            "Test MLP:  [74645] [96112]\n",
            "[0.28758792]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 69736 [96112]\n",
            "train: [2010 2011 2013 2014 2015 2016 2017], test: [2012]\n",
            "Test MLP:  [96112] [84117]\n",
            "[-0.12480231]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 69736 [84117]\n",
            "train: [2010 2012 2013 2014 2015 2016 2017], test: [2011]\n",
            "Test MLP:  [88355] [96112]\n",
            "[0.08779356]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 69736 [96112]\n",
            "train: [2010 2011 2012 2013 2014 2015 2017], test: [2016]\n",
            "Test MLP:  [77747] [74645]\n",
            "[-0.03989865]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 69736 [74645]\n",
            "train: [2011 2012 2013 2014 2015 2016 2017], test: [2010]\n",
            "Test MLP:  [87139] [77747]\n",
            "[-0.10778182]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 69736 [77747]\n",
            "train: [2010 2011 2012 2013 2015 2016 2017], test: [2014]\n",
            "Test MLP:  [90400] [77747]\n",
            "[-0.13996681]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 69736 [77747]\n",
            "train: [2010 2011 2012 2014 2015 2016 2017], test: [2013]\n",
            "Test MLP:  [93930] [77747]\n",
            "[-0.17228787]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 69736 [77747]\n",
            "train: [2010 2011 2012 2013 2014 2016 2017], test: [2015]\n",
            "Test MLP:  [84117] [96112]\n",
            "[0.142599]\n",
            "~~~ 2018 BEST MLP ~~~:  2018 69736 [96112]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}